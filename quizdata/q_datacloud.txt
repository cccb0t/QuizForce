How can attribute names be modified to match a naming convention in Cloud File Storage target?
Update attribute names in the data stream configuration
*Update field names in the data model
Set preferred attribute names when configuring activation
Use a formula field to update the field name in an activation

To import campaign members into a campaign in CRM a user wants to export the segment to Amazon S3. The resulting file needs to include CRM Campaign ID in the name. How can this outcome be achieved?
Include campaign identifier into the activation name
Hard-code the campaign identifier as a new attribute in the campaign activation
*Include campaign identifier into the filename specification
Include campaign identifier into the segment name

Which two applications automatically create activation targets at the time the application is connected to Data Cloud? (Choose 2)
*Personalization powered by Interaction Studio
Amazon S3
*B2C Commerce
Marketing Cloud Engagement

Which two steps are required when configuring a Marketing Cloud activation? (Choose 2)
Set an Activation Schedule
*Select an Activation Target
Add Additional Attributes
*Select Contact Points

What component of Calculated Insights can be included as attribute data in an activation?
Metrics and Dimensions
*Dimensions
Metrics
Filters

Which data model subject area defines the revenue or quantity for an opportunity by product family?
Engagement
Product
Party
*Sales Order

An organization wants to enable users with the ability to identify and select text attributes from a picklist of options. Which Data Cloud feature can help with this use case?
Transformation Formulas
Data Harmonization
*Value Suggestion
Global Picklists

What can be customized in the Data Cloud canonical model?
Fields
*Objects, Fields, and Relationships
Objects and Fields
Objects

Which data stream category should be assigned to use the data for time-based operations in segmentation and calculated insights?
*Transaction
Individual
Sales Order
Engagement

Every day, Northern Trail Outfitters (NTO) uploads a summary of the last 24 hours of store transactions to a new file in an Amazon S3 bucket, and files older than 7 days are automatically deleted. Each file contains a timestamp in a standardized naming convention. What should a consultant consider when ingesting this data stream?
*Ensure the refresh mode is set to Upsert and Refresh only new files is selected
Ensure the refresh mode is set to Full Refresh and the filename contains a wildcard to accommodate the timestamp
Ensure the refresh mode is set to Full Refresh and Refresh only new files' is selected
Advise NTO to change their processes: this configuration is not supported

Which two statements about Data Cloud's Web and Mobile App connector are true? (Choose 2)
Any Data Streams associated with Web or Mobile connector app will be automatically deleted upon deleting the app from Data Cloud Setup
*Data Cloud administrators can see the status of a Web or Mobile connector app on the app details page
*Tenant Specific Endpoint is auto-generated in Data Cloud when setting up a Mobile or Web app connection
Mobile and Web SDK schema can be updated to delete an existing field

Which data model subject area should be used for any Organization, Individual, or Member in the Customer 360 data model?
Individual
Global Account
*Party
Membership

Which two statements about Data Cloud's Web and Mobile App connector are true? (Choose 2)
Any Data Streams associated with Web or Mobile connector app will be automatically deleted upon deleting the app from Data Cloud Setup
*Data Cloud administrators can see the status of a Web or Mobile connector app on the app details page
*Tenant Specific Endpoint is auto-generated in Data Cloud when setting up a Mobile or Web app connection
Mobile and Web SDK schema can be updated to delete an existing field

Which two objects or fields are supported for ingestion using the Salesforce CRM connector? (Choose 2)
Custom Big Objects
Standard Big Objects
*Standard Objects
*Custom Objects

Which option allows an organization an easy way to ingest Marketing Cloud subscriber profile attributes into Data Cloud on a daily basis?
Marketing Cloud Connect API
Email Studio Starter Data Bundle
Profile attributes are not yet supported
*Automation Studio and Profile API

How does Data Cloud handle an individual's right to be forgotten?
Deletes the specified Individual and records from any DMO/DLO related to the Individual.
*Deletes the specified Individual and records from any DSO mapped to the Individual DMO.
Deletes the records from all DSOs and any downstream DMOs are updated at the next scheduled ingestion.
Deletes the specified Individual record and its Unified Individual Link record.

Cloud Kicks has received a Request to be Forgotten by a customer. In which two ways can Data Cloud honor this request? (Choose 2)
Use Data Explorer to locate and manually remove the Individual
*Use the Consent API to suppress processing and delete the individual and related records from source data streams
Delete the data from the incoming data stream and perform a full refresh
*Add the Individual Id to a headerless file and use the delete from file functionality

A customer wants to use the transactional data from their data warehouse in Data Cloud. They are only able to export the data via a SFTP site. What are two recommended ways to bring this data into Data Cloud? (Choose 2)
Manually import the file using the Data Import Wizard
Utilize Salesforce's Dataloader application to perform a bulk upload from a desktop
*Import the file into Google Cloud Storage and ingest with the Cloud Storage Connector
*Import the file into Amazon S3 and ingest with the Cloud Storage Connector

A segment fails to refresh with the error Segment references too many Data Lake Objects (DLOs). What are two remedies for this issue? (Choose 2)
*Space out the segment schedules to reduce Data Lake Object load
Refine segmentation criteria to limit up to 5 custom DMOs
*Split the segment into smaller segments
Use Calculated Insights in order to reduce the complexity of the segmentation query

Which operator can be used to create a segment for a birthday campaign that is evaluated daily?
Is This Year
*Is Anniversary Of
Is Between
Is Birthday

What is a unique requirement of a Streaming Insight query?
A dimension
A measurement
*A window function
A WHERE clause

Which three options can be used to build a filter in the Segmentation Canvas? (Choose 3)
Data Lake Objects
*Streaming Insights
*Calculated Insights
Related Attributes
*Direct Attributes

Which method should an administrator use when performing aggregations in windows of 15 minutes on data collected via the Interactions SDK and Mobile SDK?
Activation
Segment
*Streaming Insight
Calculated Insight

The leadership team at Cumulus Financial has declared that customers who have deposited more than $250,000 in the last 5 years and who are not using advisory services, will be the central focus for all new campaigns in the next year. Which two features support this need? (Choose 2)
*Calculated Insight
Report
*Segment
Dashboard

Which feature can integrate in real time with Salesforce CRM?
*Data Actions
Identity Resolution
CRM Starter Bundle
Data Model Triggers

Northern Trail Outfitters wants to be able to calculate each customer's lifetime value (LTV) but also create breakdowns of the revenue sourced by website, mobile app, and retail channels. How should this use case be addressed in Data Cloud?
Nested segments
Flow orchestration
*Streaming data transformations
Metrics on metrics

The website team at Cumulus Financial Services wants to understand which identified users have browsed the jobs page on their website at least twice within the last 12 hours. Which component should a consultant recommend to achieve this goal?
*Streaming Insight
Calculated Insight
Streaming Data Transformation
Segment

What is allowed when editing a Calculated Insight?
Removing existing measures
*Adding new measures
Adding new dimensions
Removing existing dimensions

Which three actions can be applied to a previously created segment? (Choose 3)
Reactivate
*Export
*Delete
*Copy
Inactivate

What are the two minimum requirements needed when using the Visual Insights Builder to create a Calculated Insight? (Choose 2)
WHERE clause is required
At least two objects to join
*At least one dimension
*At least one measure

What does it mean to build a trust-based, first-party data asset?
Pass the trust-based compliance rules as a first-party data asset is added to Data Cloud
*Provide transparency and security for data gathered from individuals who provide consent for its use and receive value in exchange
Obtain competitive data from reliable sources through interviews, surveys, and polls
Ensure opt-in consents are collected for all email marketing as required by law

What are the two distinct phases of data model management in Data Cloud? (Choose 2)
Data Activation
*Data Ingestion
Data Actions
*Data Modeling

Which two statements are true about using consent API and exercising right to be forgotten? (Choose 2)
Data Deletion requests are processed within 1 hour
*Data Deletion requests are reprocessed at 30, 60, and 90 days
*Data Deletion requests are submitted for Individual profiles
Data deletion requests submitted to Data Cloud are passed to all connected Salesforce Clouds

A customer has requested that their personal data be deleted. Which action should be performed to accommodate this request in Data Cloud?
Manually delete customer and related records using the Profile Explorer
*Use Consent API to request deletion of the customer's information
Utilize the Data Rights Subject Request tool to request deletion of the customer's information
Use Ingestion API to request deletion of the customer's information

What are two benefits Data Cloud provides a company in relation to managing customer data? (Choose 2)
*Unified Identity Resolution
*Data Harmonization
Data Governance
Data Marketplace

Which data sources are available from Marketing Cloud as a starter bundle?
Email, Cloud Pages, Einstein Web & Email Recommendations
Email, MobileConnect, MobilePush and GroupConnect
Email, Mobile Connect, and Einstein Engagement Scoring
*Email, MobileConnect and MobilePush

Which two features are impacted by the timezone setting in the org Data Cloud is provisioned in? (Choose 2)
*Segment Schedule
Identity Resolution
Ingestion Schedule
*Activation Schedule

Which three out-of-the-box connectors are available for Data Cloud? (Choose 3)
*Marketing Cloud
*B2C Commerce
Slack Connector
*Amazon S3
Amazon Redshift Connector

Which permission setting should an administrator check if the custom CRM object is not available in New Data Stream configuration?
Modify All object permission enabled in Data Cloud org
Ingest Object permission is enabled in the CRM org
Create object permission enabled in Data Cloud org
*View All object permission enabled in source CRM org

What is the first thing a business stakeholder should focus on when considering a Data Cloud implementation?
Review consent and privacy management policies
*Obtain cross-organizational buy-in
Identify activation targets
Identify data sources

Which three features can be used to validate the data in the unified profile object? (Choose 3)
Data Actions
*Data Explorer
*Query API
*Profile Explorer
Identity Reconciliation

Which configuration can support separate Amazon S3 buckets for data ingestion and activation?
Dedicated S3 data sources in activation setup
*Dedicated S3 data sources in Data Cloud setup
Separate user credentials for data stream and activation
Separate user credentials for data stream and activation target

A user needs permissions to access Data Cloud to create, manage, and publish segments. However, the user should not be allowed to create reports or manage data sources. Which permission set should an administrator assign?
Customer Data Cloud for Marketing Admin
*Customer Data Cloud for Marketing Specialist
Customer Data Cloud for Marketing Data Aware Specialist
Customer Data Cloud for Marketing Manager

Which three components of Data Cloud can be bundled within a Data Kit? (Choose 3)
*Data Models
Calculated Insights
*Segments
*Data Streams
Identity Resolution Rulesets

An organization is looking to use Data Cloud to unify data across 5 Salesforce orgs, 2 Marketing Cloud accounts, 6 Amazon S3 Buckets, and 4 Personalization datasets. As far as connection limits are concerned, which platform is going to present a challenge?
Amazon S3 Bucket
Personalization
*Marketing Cloud
Salesforce CRM

How does an administrator increase the consolidation rate for Identity Resolution?
Change all reconciliation rules to Source Sequence
Add more matching rules to broaden the search for matches
Change the Ignore Empty Value option
*Reduce the number of matching rules

Which three Data Model objects do Reconciliation Rules operate across? (Choose 3)
*Individual
*Party Identification
*Contact Point Email
Contact
Lead

Which match criteria is only available as custom in order to perform an exact match in Identity Resolution?
Phone Number
Email Address
*Party Identification Id
Government Id

How does Identity Resolution select attributes for Unified Individuals when there is conflicting information in the Data Model?
Create additional Rule Sets
*Leverage Reconciliation Rules
Create additional Contact Points
Leverage Match Rules

What is the relationship between Individual and Contact Point Objects?
1:1
*1: Many
Many: Many
None of the above

Which of these is present in Identity Resolution Summary?
Unified Individuals
Last Processing Status
Matched Individuals
*All the above

When can the data types be changed during ingestion?
After the DSO is created
*Before DSO is created
Data type can never be changed
Data type can be changed at any time

Which of the following functions returns the first value from a list that isn't empty?
CONCAT
*COALESCE
REPLACE
FIND

How many days of historical data is loaded when B2C Commerce Data Stream is created from the Order Bundle?
90 days
60 days
*30 days
10 days

What is the maximum limit of the number of records for Full Refresh Extract Method?
*10 million
40 million
50 million
No hard limit

Which type of measures with aggregate functions are supported?
Date
Text
*Numeric
Boolean

How many Metrics can be present in 1 segment container?
*1
5
10
100

How many calculated insights can be created per tenant?
5
10
*50
100

Which of the following cannot be used in Segmentation? (Choose 2)
Numeric Measures
*Text Measures
Aggregate Functions
*Date Time Measures

Which of the following are characteristics of Formulas? (Choose 2)
*Simple logic on a row-based operation
*Ease of use, self-service
Highly reusable content
Attribute updated regularly

What is a DMO called if it inherits the name, shape, and semantics of the reference object?
Custom DMO
*Standard DMO
Data Lake Object (DLO)
Data Source Object (DSO)

How do the Data streams that use the Marketing Cloud Connector refresh data?
Manually refreshed by CDP Admin
*Marketing Cloud's Automation Studio handles this process.
APIs are used for refresh
None of the above

What should be the type of the Event Time Field while ingesting Engagement data?
Mutable
Inconsistent
*Immutable
None of the above

What happens if no file name is specified in AWS S3 data stream during ingestion?
*The system does not fetch any file and the data stream shows an error.
The system chooses the first file found in the S3 bucket
The ingestion setup can't be completed without specifying the filename.
The ingestion setup is completed but the data stream shows 0 records

A customer wants to create segments of users based on their Customer Lifetime Value. The source data that will be brought into the Data Cloud does not include that KPI. What process should be followed to achieve this outcome?
Map Data to Data Model > Create Calculated Insight > Use in Segmentation
*Ingest Data > Map Data to Data Model > Create Calculated Insight
Insight > Use in Segmentation
Ingest Data > Create Calculated Insight > Use in Segmentation
Create Calculated Insight > Map Data to Data Model > Use in Segmentation

Which two common use cases can be addressed with Data Cloud? (Choose 2)
*Harmonize data from multiple sources with a standardized and extendable data model
*Understand and act upon customer data to drive more relevant experiences
Safeguard critical business data by serving as a centralized system for backup and disaster recovery
Govern enterprise data lifecycle through a centralized set of policies and processes

What is Data Cloud's primary value to customers?
A single source of truth for all anonymous data
A platform that can update all connected systems with a golden record in real-time
*A platform that provides a unified view of a customer and their related data
A platform that can create personalized campaigns by listening, understanding, and acting on customer behavior

Which two objects or fields are supported for ingestion using the Salesforce CRM connector?
*Standard Objects
Standard Big Objects
Custom Big Objects
*Custom Objects

An administrator is setting up a data stream with transactional data. What field type should the administrator choose to ensure that leading zeros in the purchase order number are preserved?
Number
Decimal
*Text
Serial

A customer has a custom 'Customer_Email_c' object related to the standard 'Contact' object in Salesforce CRM. To which data entity is this mapped?
Contact
*Contact Point Email
Custom 'Customer_Email' Object
Individual

Which two characteristics describe the Customer 360 data model? (Choose 2)
*A Conceptual Model
A Static Model
A Product Sold By Salesforce
*A Canonical Model

When setting up the data source object or schema for data ingestion, what are the three data categories to select from? (Choose 3)
*Engagement Data
Event Data
Other Data
*Order Data
*Profile Data

To which Data Model entity should the Email field from a CRM Contact object be mapped?
Lead
Account Contact
Individual
*Contact Point Email

What is the correct formula to display the value of the raw data column of RetailPrice' plus an additional 5 percent?
sourceField['RetailPrice1*l .05
SELECT(['RetailPrice']*1.05)
*sourceField['retailprice']*1.05
SELECT(['retailprice']*1.05)

Which two dependencies need to be removed prior to disconnecting a data source? (Choose 2)
Activation Target
*Data Stream
*Segment
Activation

What is the first step to set up and configure a Data Cloud instance after it has been provisioned?
Complete the Salesforce Data Cloud Get Started'' process
Connect to the Marketing Cloud Account Data Cloud is provisioned in
Enable Customer Data Cloud Admin permission set to the relevant Salesforce CRM user
*Connect to the Salesforce CRM org Data Cloud is provisioned in

When performing Segmentation or Activation, which timezone is used to publish and refresh data?
Timezone of the Data Cloud Admin user
Timezone is explicitly specified when creating a segment or activation
*Timezone set by the Salesforce Data Cloud org
Timezone of the user defining the activity

An administrator has configured the Salesforce CRM connector and set up a data stream for the Case object. A new custom field called Business Priority was created on the Case object. However, that field is not available when trying to add it in the data stream. What could be causing this issue?
Custom fields on the Case objects are not supported for ingesting into Data Cloud
Utilize the Salesforce Dataloader application to perform a bulk upload from a desktop
The Data Cloud administrator does not need to do anything. After 24 hours when the data stream refreshes, it will automatically include any new fields that were added to CRM
*The Salesforce Integration User is missing Read permissions on the newly created field

Which two dependencies can prevent a Data Stream from being deleted? (Choose 2)
*A data stream attribute is mapped to a Data Model object
*A data stream attribute is used in Calculated Insight
A data stream attribute is used in Segmentation
A data stream attribute is used in Activation

A retail customer wants to bring customer data from different sources and wants to take advantage of Identity Resolution so that it can be used in Segmentation. On which entity should this be segmented for activation membership?
Subscriber
Unified Contact
*Unified Individual
Individual

Which authentication type is supported for a Cloud File Storage activation target?
Using private key certificate
*Using access and secret keys
Using encrypted username and password
Using JWT token

Northern Trail Outfitters uploads new customer data to an Amazon S3 Bucket on a daily basis to be ingested in Data Cloud. In what order should each process be run to ensure that freshly imported data is ready and available to use for any segment?
Refresh Data Stream > Calculated Insight > Identity Resolution
Identity Resolution > Calculated Insight > Refresh Data Stream
Calculated Insight > Identity Resolution > Refresh Data Stream
*Refresh Data Stream > Identity Resolution > Calculated Insight

What does the Ignore Empty Value option do in Identity Resolution?
Ignores Individual object records with empty fields when running Identity Resolution rules
Ignores empty fields when running any custom match rules
*Ignores empty fields when running reconciliation rules
Ignores empty fields when running the standard match rules

Which three objects are created as a result of Identity Resolution? (Choose 3)
Unified Subscriber
Unified Data Model
*Unified Contact Point
*Unified Link
*Unified Individual

What does the Source Sequence reconciliation rule do in Identity Resolution?
*Sets the priority of specific data sources when building attributes in a unified profile such as a first or last name
Identifies which data sources should be used in the process of reconciliation by prioritizing the most recently updated data source
Includes data from sources where the data is alphanumerically sequenced
Identifies which individual records should be merged into a unified profile by setting a priority for specific data sources

When creating a segment on an individual, what is the result of using two separate containers linked by an AND: At Least 1 of GoodsProduct.Color Is Equal To 'red' AND At Least 1 of GoodsProduct.PrimaryProductCategory Is Equal To shoes'?
*Individuals who purchased at least 1 of any red' product and also purchased at least 1 pair of shoes'
Individuals who purchased at least 1 'red shoes' as a single line item in a purchase
Individuals who purchased at least 1 'red shoes'. 1 of any red' item, or 1 of any 'shoes' item in a purchase
Individuals who made a purchase of at least 1 of only 'red shoes' and nothing else

Which data model object category can a Data Cloud user create segments on?
Profile
*Unified Individual only
Engagement
Other

What are three benefits of Calculated Insights over Segmentation Operators? (Choose 3)
*Calculated Insights are better suited for non-trivial calculations, such as calculating a Net Promoter Score as a percentage
Calculated Insights results can be refreshed near real time
Calculated Insights are better suited for single row based operation
*Calculated Insights can query engagement data greater than 2 years
*Calculated Insights are better suited for complex queries over multiple objects

What is the result of a segmentation criteria filtering on City | Is Equal To I 'San Jose?
Cities containing 'San Jose', 'San Jose', 'san jose'. or 'san jose'
Cities only containing 'San Jose' or 'San Jose'
*Cities only containing 'San Jose' or 'san jose'

An administrator wants to be able to create a multi-dimensional metric to identify unified individual lifetime value (LTV). Which sequence of DMO joins are necessary within the Calculated Insight to enable this calculation?
Unified Individual > Individual > Sales Order
*Unified Individual > Unified Link Individual > Sales Order
Sales Order > Unified Individual
Sales Order > Individual > Unified Individual

What should an administrator do to pause a segment activation but with the intent of using that segment again?
Inactivate the segment
Delete the segment
*Stop the Publish Schedule
Skip the Activation

Cumulus Financial wants to be able to track the daily transaction volume for of each of its customers in real time and send out a notification as soon it detects volume outside a customer's normal range. How should an administrator accommodate this request?
Use Streaming Data Transformations with a Flow
*Use a Streaming Insight paired with a Data Action
Use Streaming Data Transformations combined with a Data Action
Use a Calculated Insight paired with a Flow

When authoring streaming insights via structured query language (SQL), which activities do you perform? Choose two.
*Mapping to real-time sources
*Using data actions
Creating timestamps in Data Cloud

What are three supported data-action targets for Salesforce Data Cloud streaming insights?
*Salesforce Marketing Cloud
*Webhook
*Salesforce Platform Event
Salesforce Commerce Cloud
Slack

A customer is interested in updating the CRM contact lead based on streaming data that’s coming to Data Cloud. Which two features would meet this requirement?
*Data actions
Activate to CRM
CRM Connector
*Copy fields

What data points can be retrieved from Calculated Insights when activating a segment?
*Metrics
Dimensions
Metrics and Dimensions
Related Attributes

What are the different ways to create a Calculated Insight?
*Using Insights Builder
Import query activity from Marketing Cloud
*Using structured query language
Import Salesforce object query language from developer console

Which two statements are true for Value Suggestions?
*Only text attributes can be enabled.
*“Enable Value Suggestion” needs to be enabled.
A value with more than 15 characters isn’t possible.
Attribute values are displayed in date order with the most recent displaying first. 

If you want to use the same criteria in multiple segments, which feature would you recommend?
Segment membership data model object
*Nested segments
Segment exclusions
Segmentation API

When activating an unified individual to Marketing Cloud, which three attributes are automatically included? Select three.
*Email subscriber key
*Contact email address
*Contact phone number
Contact address
Contact ID

Which type of insight is best suited to analyze clickstream data every five minutes?
Calculated insights
*Streaming insights
Einstein insights
Inline list insights

Which three types of usages are Calculated Insights best suited for?
*Nontrivial calculation
*Complex queries across multiple objects
*Reusability purposes
Data collected in batches
High volume of data processing

How can you view details on segment membership, including which users are in a specific segment?
It’s not possible to access details on segment membership.
Open the segmentation canvas and review details on who is in a specific segment.
*View the segment membership data model object data via Data Explorer.
Download the segment membership data via the download feature.

Attributes with which data type support value suggestion?
Date
Number
*Text
All types

Which attributes in the attribute library have a one-to-many relationship with the segment target?
*Related attributes
Engagement attributes
Profile attributes
Direct attributes

Which entity type(s) can be used to “Segment On” in Segmentation?
*Profile
Engagement
Profile and Engagement
Profile, Engagement, and Other

Which three types of Data Cloud metadata can be accessed via Salesforce Flow for orchestration?
*Data stream ingestion
Data model mapping
*Calculated insights publish
*Publish identity resolution job
Initiate a streaming transform

What makes segments available only to certain groups based on their role?
Salesforce Metadata API 
*Sharing Rules
Lightning App Builder
Lightning Report Builder

Which three Data Cloud actions are available via Flow?
*Publish Calculated Insights
*Publish Segments
*Trigger Identity Resolution Job
Map Data Model Object Fields
Publish Streaming Insights

Which of the following attributes describe a package? Choose three.
*A container that’s distributed
A framework to custom-tie metadata
*If managed, then used by an independent software vendor on AppExchange
Allows more control in the package content
*May include data streams (S3), data models, and calculated insights

What receives notifications about the latest status of Data Cloud usage without users viewing records?
Salesforce Metadata API 
Sharing Rules
Lightning Report Builder
*Salesforce Flow

What can be used to import AWS Data Streams into Salesforce without using the Data Cloud Product interface?
*Salesforce Metadata API 
Lightning App Builder
Lightning Report Builder
Salesforce Flow

Which of the following are features of an unmanaged package? Choose three.
*It’s not upgradeable or supported.
It’s typically used by independent software vendors on AppExchange.
*It’s commonly used for one-time migration of metadata.
It contains a namespace.
*A package developer has no control over the components once installed.
It’s upgradeable and supported.

How can a user ensure that their Data Cloud dashboards are refreshed on a daily basis?
*Select Subscribe and set a refresh window for after the data streams are refreshed.
Configure the Dashboard settings in the org setup screen.
Disable and republish the dashboard.
Schedule a refresh window for each report individually.

Identify the three objects that are supported by Sharing Rules.
Data Models
*Calculated Insights
*Segments 
*Activation Targets
Unified Individuals

Which Permission Set manages the overall segmentation strategy and identifies the target campaigns?
IT manager
*Marketing manager
Data-aware specialist
Marketing specialist

Which tab in the navigation manages the data coming into Data Cloud?
Segments
*Data streams
Activation
Data model

What must the Admin user do first when setting up users in Data Cloud?
Assign permission sets
Setup each user as an admin
*Create profiles for each user role
Configure data sources

Which of the following reflects the correct order of the Data Cloud Setup process flow?
*Configure Admin user, provision and complete Data Cloud setup, configure additional users & permissions, and connect to relevant Salesforce Clouds
Connect to relevant Salesforce Clouds, provision and complete Data Cloud setup, configure Admin user, and configure additional users and permissions
Provision and complete Data Cloud setup, connect to relevant Salesforce Clouds, and configure Admin user
Configure additional users and permissions, configure Admin user, provision and complete Data Cloud setup, and Connect to relevant Salesforce Clouds

Which connection can a Data Aware Specialist setup to ingest data from without needing the Admin to explicitly setup the connection?
Google Cloud Storage
B2C Commerce
*Amazon S3
Salesforce CRM

When using the GCS Connector, how frequently is data from Google Cloud Storage synchronized with Data Cloud?
Every 15 minutes
*Every 1 hour
Every 12 hours
Every 24 hours

What 2 scenarios would you recommend when provisioning Data Cloud in an existing CRM Data Org? 
Existing CRM Data Org has been highly customized
*Customer data is housed in a single Salesforce Org
*Customer is using Loyalty Management and Promotions
Customer has a need to connect multiple CRM Orgs

Which permission set is required to setup an External Activation Platform? 
*Customer Data Platform Admin
Customer Data Platform Data Aware Specialist
Customer Data Platform Marketing Manager
Customer Data Platform Marketing Specialist

What is used to customize and organize Data Cloud Unified Individual Record Pages for surfacing insights?
Salesforce Metadata API 
Sharing Rules
*Lightning App Builder
Lightning Report Builder
Salesforce Flow

Identify a typical use case for customizing and organizing the Data Cloud Unified Individual record page.
So only specific teams can view a specific record page
*To order and hide specific fields that are relevant to a team
To reduce potential metadata duplications
To trigger an action when a change is made

What can be used to make Segments available only to certain groups based on their role?
Salesforce Metadata API 
*Sharing Rules
Lightning App Builder
Lightning Report Builder
Salesforce Flow

Identify the features of an Unmanaged Package. Choose three.
*Not upgradeable or supported
Typically used by ISVs on AppExchange
*Commonly used for one-time migration of metadata
Contains a namespace
*Package Developer has no control over the components once installed
Is upgradeable and supported

Which three Data Cloud actions are available via Flow?
*Publish Calculated Insights
*Publish Segments
*Trigger Identity Resolution Job
Map Data Model Object Fields
Publish Streaming Insights

What cannot be used to create a report on Data Cloud data that’s in a DMO (Data Model Objects)?
Tableau
CRM Analytics
*CRM Reports and Dashboards
Marketing Cloud Intelligence

If you want to build a CRM report to include a Data Cloud configuration object, what do you need to do first?
*Configure a custom report type
Customize the object
Import metadata for the object
Create a sharing rule to make the object availableA consultant is setting up a data stream with transactional data. Which field type should the consultant choose to ensure that leading zeros in the purchase order number are preserved?
*Number
Decimal
Serial
Text
Explanation: Text fields are suitable for preserving leading zeros in alphanumeric values like purchase order numbers. Unlike numerical fields, text fields treat the entry as a sequence of characters rather than a numerical value, thereby maintaining any leading zeros in the data. This is crucial for preserving the integrity of purchase order numbers where the presence of leading zeros might be significant for identification or system processing purposes.

A customer is concerned that the consolidation rate displayed in the identity resolution is quite low compared to their initial estimations. Which configuration change should a consultant consider in order to increase the consolidation rate?
Include additional attributes in the existing matching rules.
Reduce the number of matching rules.
*Increase the number of matching rules
Change reconciliation rules to Most Occuring.
Explanation: Adding more matching rules allows for a more granular comparison of attributes, potentially leading to better identification and consolidation of records. It enhance the consolidation rate in identity resolution by providing more criteria for accurate matching.

Every day, Northern Trail Outfitters uploads a summary of the last 24 hours of store transactions to a new file in an Amazon S3 bucket, and files older than seven days are automatically deleted. Each file contains a timestamp in a standardized naming convention. Which two options should a consultant configure when ingesting this data stream? Choose 2 answers.
*Ensure the refresh mode is set to "Upsert"
Ensure the refresh mode is set to "Full Refresh".
Ensure that deletion of old files is enabled.
*Ensure the filename contains a wildcard to accommodate the timestamp
Explanation: <strong>Ensure the filename contains a wildcard to accommodate the timestamp.</strong> This is crucial to allow the system to recognize and ingest files based on the varying timestamps present in the filenames. <strong>Ensure the refresh mode is set to "Upsert".</strong> "Upsert" mode helps in updating existing records and inserting new ones. In this scenario, it might not directly relate to the deletion of old files but allows for efficient data updates in the system if changes occur in the existing files. <br>

A consultant has an activation that is set to publish every 12 hours, but has discovered that updates to the data prior to activation are delayed by up to 24 hours. Which two areas should a consultant review to troubleshoot this issue? Choose 2 answers
*Review segments to ensure they're refreshed after the data is ingested.
*Review data transformations to ensure they're run after calculated insights.
Review calculated insights to make sure they're run before segments are refreshed.
Review calculated insights to make sure they're run after the segments are refreshed.

Which statement about Data Cloud's Web and Mobile Application Connector is true?
*A standard schema containing event, profile, and transaction data is created at the time the connector is configured.
The Tenant Specific Endpoint is auto-generated in Data Cloud when setting the connector.
The connector schema can be updated to delete an existing field.
Any data streams associated with the connector will be automatically deleted upon deleting the app from Data Cloud Setup.
Explanation: This connector typically establishes a structured schema that encompasses various types of data related to events, profiles, and transactions from web and mobile applications. This standardized schema formation occurs during the setup of the connector to facilitate the integration and ingestion of these different types of data into Data Cloud.

What does the Ignore Empty Value option do in identity resolution?
*Ignores empty fields when running reconciliation rules
Ignores Individual object records with empty fields when running identity resolution rules
Ignores empty fields when running the standard match rules
Ignores empty fields when running any custom match rules
Explanation: This option specifies that empty fields are ignored specifically during the reconciliation rules process. Reconciliation rules are used to resolve conflicts or discrepancies between different data sources, and ignoring empty fields during this phase can help improve the accuracy of the reconciliation by focusing on non-empty or relevant data for matching and merging purposes.

A user is not seeing suggested values from newly-modeled data when building a segment. What is causing this issue?
Value suggestion requires Data Aware Specialist permissions at a minimum.
Value suggestion will only return results for the first 50 values of a specific attribute.
Value suggestion can only work on direct attributes and not related attributes.
*Value suggestion is still processing and takes up to 24 hours to be available.
Explanation: When new data is modeled or added to the system, it often needs time to undergo processing and indexing before value suggestions become available. This processing period might take up to 24 hours before the system can suggest values for attributes that have been recently added or modified. During this processing time, the system is organizing and indexing the data, making it ready for use in features like value suggestion. So, if a user isn't seeing suggestions for values with newly-modeled data, it could be because the system is still processing the information, and it may take some time before the suggestions become available for use in segmentation.

A customer requests that their personal data be deleted. Which action should the consultant take to accommodate this request in Data Cloud?
Use Consent API to request deletion of the customer's information.
*Use the Data Rights Subject Request tool to request deletion of the customer's information.
Use Profile Explorer to delete the customer data from Data Cloud.
Use a streaming API call to delete the customer's information.
Explanation: This tool is designed specifically for handling data subject requests, such as requests for data deletion. It allows the consultant to manage and process requests related to data rights, including deletion requests, in accordance with privacy regulations and company policies. Using this tool ensures that the customer's request is appropriately handled within the DataCloud system.

Northern Trail Outfitters (NTO) is configuring an identity resolution ruleset based on Fuzzy Name and Normalized Email. What should NTO do to ensure the best email address is activated?
Set the default reconciliation rule to Last Updated.
Include Contact Point Email object Is Active field as a match rule.
Ensure Marketing Cloud is prioritized as the first data source in the Source Priority reconciliation rule.
*Use the source priority order in activations to make sure a contact point from the desired source is delivered to the activation target.

The Salesforce CRM Connector is configured and the Case object data stream is set up. Subsequently, a new custom field named Business Priority is created on the Case object in Salesforce CRM. However, the new field is not available when trying to add it to the data stream. Which statement addresses the cause of this issue?
Custom fields on the Case object are not supported for ingesting into Data Cloud
The Salesforce Data Loader application should be used to perform a bulk upload from a desktop
*The Salesforce Integration User is missing Read permissions on the newly created field
After 24 hours when the data stream refreshes, it will automatically include any new fields that were added to the Salesforce CRM.
Explanation: If the Integration User doesn't have appropriate permissions to access the newly created field, it won't be available for addition to the data stream within Data Cloud. Checking and adjusting the permissions for the Salesforce Integration User on the "BusinessPriority" field could resolve this issue.

Which two requirements must be met for a calculated insight to appear in the segmentation canvas? Choose 2 answers
The primary key of the segmented table must be a metric in the calculated insight.
*The primary key of the segmented table must be a dimension in the calculated insight.
*The calculated insight must contain a dimension including the Individual or Unified Individual Id.
The metrics of the calculated insights must only contain numeric values.
Explanation: For a calculated insight to appear in the segmentation canvas, it needs to contain certain dimensions and requirements: The calculated insight must contain a dimension including the Individual or UnifiedIndividualId. This is crucial for associating the insights with specific individuals within the segmentation. The primary key of the segmented table (the table storing the calculated insights' data) must be a dimension in the calculated insight. This linkage allows DataCloud to align the data from the insights with the segmentation canvas effectively, ensuring that the insights can be appropriately utilized within the segmentation tool.

A consultant is reviewing a recent activation using engagement-based related attributes but is not seeing any related attributes in their payload for the majority of their segment members. Which two areas should the consultant review to help troubleshoot this issue? Choose 2 answers.
*The activated profiles have a Unified Contact Point.
*The correct path is selected for the related attributes.
The activations are referencing segments that segment on profile data rather than engagement data.
The related engagement events occurred within the last 90 days.

A customer notices that their consolidation rate has recently increased. They contact the consultant to ask why. What are two likely explanations for the increase? Choose 2 answers
Duplicates have been removed from source system datas treams
*New data sources have been added to Data Cloud that largely overlap with the existing profiles.
Identity resolution rules have been removed to reduce the number of matched profiles.
*Identity resolution rules have been added to the rule set to increase the number of matched profiles.
Explanation: A. <strong>New data sources have been added to Data Cloud that largely overlap with the existing profiles.</strong> Explanation: When new data sources are incorporated into a Data Cloud environment and these sources contain information similar to what's already present in the system, there's a higher chance of finding matches or overlaps between the existing profiles and the new data. This increased overlap leads to a higher consolidation rate as more data can be integrated or associated with existing customer profiles, thereby enhancing the consolidation process. D. <strong>Identity resolution rules have been added to the ruleset to increase the number of matched profiles.</strong> Explanation: Identity resolution rules play a critical role in determining how data is matched and consolidated within a system. When new or refined rules are introduced, they can be designed to find more connections between different pieces of data, resulting in a higher number of matches between profiles. This leads to an increased consolidation rate as more data is correctly attributed to existing customer profiles based on these updated rules.

Cumulus Financial uses Data Cloud to segment banking customers and activate them for direct mail via a Cloud File Storage activation. The company also wants to analyze individuals who have been in the segment within the last 2 years. Which Data Cloud component allows for this?
Nested segments
Calculated insights
Segment exclusion
*Segment membership data model object
Explanation: In some data management systems, there are entities or data structures dedicated to recording membership or affiliation of individuals with various segments or groups. These objects store information about which individuals are part of specific segments or categories. In the scenario where CumulusFinancial wants to analyze individuals who have been in a segment within the last 2 years, a "Segment membership data model object" might indeed hold relevant information. If this object captures historical data regarding when individuals joined or left specific segments, it could facilitate the analysis required by CumulusFinancial to identify individuals who were part of a segment within the specified time frame.

Cumulus Financial wants to segregate Salesforce CRM Account data based on Country for its Data Cloud users. What should the consultant do to accomplish this?
Use the data spaces feature and apply filtering on the Account data lake object based on Country.
Use formula fields based on the Account Country field to filter incoming records.
Use streaming transforms to filter out Account data based on Country and map to separate data model objects accordingly.
*Use Salesforce sharing rules on the Account object to filter and segregate records based on Country.

Which information is provided in a .csv file when activating to Amazon S3?
The manifest of origin sources within Data Cloud.
*The activated data payload
The metadata regarding the segment definition
An audit log showing the user who activated the segment and when it was activated
Explanation: When a segment is activated to Amazon S3, the .csv file contains the actual data payload from the segment that has been processed and is ready for use or further analysis. This file includes the information that meets the criteria of the segment and has been selected for activation.

Data Cloud receives a nightly file of all ecommerce transactions from the previous day. Several segments and activations depend upon calculated insights from the updated data in order to maintain accuracy in the customer's scheduled campaign messages. What should the consultant do to ensure the ecommerce data is ready for use for each of the scheduled activations?
*Use Flow to trigger a change data event on the ecommerce data to refresh calculated insights and segments before the activations are scheduled to run.
Ensure the segments are set to Rapid Publish and set to refresh every hour.
Ensure the activations are set to Incremental Activation and automatically publish every hour.
Set a refresh schedule for the calculated insights to occur every hour.
Explanation: By using Flow to initiate a change data event on the e-commerce data store, the consultant can trigger the recalculation or refresh of calculated insights and segments before the scheduled activations. This ensures that the insights and segments are updated with the latest e-commerce data, maintaining accuracy and relevance for each scheduled activation. This proactive approach guarantees that the activations utilize the most recent and accurate data when executing their campaigns.

Northern Trail Outfitters uploads new customer data to an Amazon S3 Bucket on a daily basis to be ingested in Data Cloud. In what order should each process be run to ensure that freshly imported data is ready and available to use for any segment?
RefreshData Stream&gt;CalculatedInsight&gt;IdentityResolution
CalculatedInsight&gt;Refresh DataStream&gt;IdentityResolution
*RefreshDataStream&gt;IdentityResolution&gt;CalculatedInsight
IdentityResolution&gt;RefreshData Stream&gt;CalculatedInsight
Explanation: To ensure that freshly imported data is ready and available for any segment in DataCloud, the following order of processes should be executed:</p><ol><li><p><strong>Refresh DataStream</strong>: This step involves bringing in the newly uploaded customer data from the Amazon S3 bucket into the DataCloud. It's the initial stage where the raw data is updated and made available for processing.</p></li><li><p><strong>IdentityResolution</strong>: Once the new data is ingested, the Identity Resolution process should follow. This step aims to match and consolidate customer profiles based on the updated data, ensuring that the identities are correctly resolved and associated.</p></li><li><p><strong>CalculatedInsight</strong>: Lastly, Calculated Insight processes should occur after the Identity Resolution is complete. This step involves deriving insights or creating calculated fields based on the consolidated data, making it ready for segment creation or any other analytics-driven tasks.</p></li></ol></div></div></form></div><div class="detailed-result-panel--panel-row--4lvVX detailed-result-panel--question-container--vh1KF"><form class="mc-quiz-question--container--dV-tK"><span>

A customer has multiple team members who create segment audiences that work in different time zones. One team member works at the home office in the Pacific time zone, that matches the org Time Zone setting. Another team member works remotely in the Eastern time zone. Which user will see their home time zone in the segment and activation schedule areas?
Neither team member; Data Cloud shows all schedules in GMT.
The team member in the Eastern time zone.
*Both team members; Data Cloud adjusts the segment and activation schedules to the timezone of the logged-in user.
The team member in the Pacific time zone.
Explanation: DataCloud typically adjusts the segment and activation schedules to match the timezone of the user who is logged in and creating or managing those segments. This accommodation ensures that users working in different time zones can see and work with schedules and segments in their respective local times, providing convenience and accuracy in their operations within DataCloud.

A customer wants to create segments of users based on their Customer Lifetime Value. However, the source data that will be brought into Data Cloud does not include that key performance indicator (KPI). Which sequence of steps should the consultant follow to achieve this requirement?
Create Calculated Insight&gt; Map Data to Data Model&gt;Ingest Data&gt;Use in Segmentation
Ingest Data &gt;Create Calculated Insight &gt;Map Data to Data Model &gt;Use in Segmentation
Create Calculated Insight &gt;Ingest Data &gt;Map Data to Data Model&gt;Use in Segmentation
*Ingest Data&gt;Map Data to Data Model&gt;Create Calculated Insight&gt;Use in Segmentation

Cumulus Financial uses calculated insights to compute the total banking value per branch for its high net worth customers. In the calculated insight, "banking value" is a metric, "branch" is a dimension, and "high net worth" is a filter. What can be included as an attribute in activation?
*"branch"(dimension)
"branch" (dimension) and "banking value" (metric)
"banking value"(metric)
"high net worth" (filter)
Explanation: Attributes usually represent dimensions or categorical values that help in segmenting or filtering data. The "branch" dimension can be included as an attribute in activation as it allows the segmentation or grouping of data based on different branches, enabling targeted actions or campaigns for each specific branch within CumulusFinancial's high net worth customer base.

What is Data Cloud's primary value to customers?
To connect all systems with a golden record
To create a single source of truth for all anonymous data
To create personalized campaigns by listening,understanding, and acting on customer behavior
*To provide a unified view of a customer and their related data
Explanation: The primary value of a Data Cloud to customers generally aligns with: A. <strong>To provide a unified view of a customer and their related data</strong> Data Cloud solutions aim to aggregate and integrate data from various sources to create a comprehensive, unified view of customers or entities. This unified view helps organizations gain insights into customer behaviour, preferences, and interactions across different touchpoints. It allows for better decision-making, personalized experiences, and more effective engagement strategies based on a holistic understanding of the customer.

A consultant is working in a customer's Data Cloud org and is asked to delete the existing  identity resolution rule set. Which two impacts should the consultant communicate as a result of this action? Choose 2 answers
All source profile data will be removed.
*Dependencies on data model objects will be removed.
*Unified customer data associated with this ruleset will be removed.
All individual data will be removed.
Explanation: When deleting an existing identity resolution ruleset in Data Cloud, the impacts would typically be as follows: <strong>Unified customer data associated with this ruleset will be removed.</strong> Since the ruleset contributes to the unification of customer data, deleting it can result in the loss of the unified view of customer information. <strong> Dependencies on data model objects will be removed.</strong> Data model objects often rely on or have dependencies on certain rulesets for data unification. Deleting a ruleset might affect these dependencies and their association with data model objects.

A customer has a calculated insight about lifetime value. What does the consultant need to be aware of if the calculated insight needs to be modified?
*New dimensions can be added.
Existing measures can be removed.
New measures can be added
Existing dimensions can be removed.
Explanation: Because adding new dimensions provides additional perspectives or attributes to the existing insight, potentially enriching the analysis by incorporating different angles or factors related to customer lifetime value. <br>

Which operator should a consultant use to create a segment for a birthday campaign that is evaluated daily?
*Is Anniversary Of
Is Birthday
Is Between
Is Today
Explanation: This operator allows for the evaluation of whether a specific date is the anniversary of another date, making it ideal for targeting individuals based on their birth dates as anniversaries within the context of the campaign.

The recruiting team at Cumulus Financial wants to identify which candidates have browsed the jobs page on its website at least twice within the last 24 hours. They want the information about these candidates to be available for segmentation in Data Cloud and the candidates added to their recruiting system. Which feature should a consultant recommend to achieve this goal?
*Streaming insight
Calculated insight
Streaming data transform
Batch bata transform
Explanation: Streaming insights offer real-time data analysis and identification of patterns or behaviors as they occur. In this scenario, it enables the identification of candidates meeting specific browsing criteria in real-time (at least two visits to the jobs page within 24 hours) and allows for immediate segmentation within Data Cloud. This real-time insight can then be integrated into the recruiting system for quick and dynamic candidate updates. <br>

What does it mean to build a trust-based, first-party data asset?
To provide trusted, first-party data in the Data Cloud Marketplace that follows all compliance regulations.
*To provide transparency and security for data gathered from individuals who provide consent for its use and receive value in exchange.
To obtain competitive data from reliable sources through interviews, surveys, and polls.
To ensure opt-in consents are collected for all email marketing as required by law
Explanation: This process involves collecting data directly from individuals who willingly consent to sharing their information. It prioritizes transparency about how their data is being used, ensuring security measures to protect their information, and offering value or benefits in return for sharing their data. This approach aims to establish trust between the data collector and the individuals providing their information, fostering a mutually beneficial relationship.

Northern Trail Outfitters wants to implement Data Cloud and has several use cases in mind. Which two usecases are considered a good fit for Data Cloud?
To create and orchest rate cross-channel marketing messages
*To ingest and unify data from various sources to reconcile customer identity
*To use harmonized data to more accurately understand the customer and business impact
To eliminate the need for separate business intelligence and IT data management tools
Explanation: For Northern Trail Outfitters' use cases, the following two options are considered good fits for Data Cloud implementation: A. <strong>To ingest and unify data from various sources to reconcile customer identity</strong> Data Cloud solutions are well-suited for ingesting data from multiple sources and unifying it to create a cohesive and accurate view of customer identity. This helps in understanding and managing customer profiles effectively. C. <strong>To use harmonized data to more accurately understand the customer and business impact</strong> Utilizing harmonized data to gain insights into customer behavior and its impact on the business aligns with the core capabilities of Data Cloud solutions. This comprehensive understanding aids in making informed decisions and deriving actionable insights.

A consultant wants to build a new audience in Data Cloud. Which three criteria can the consultant include when building a segment? Choose 3 answers
Calculated insights
Streaming insights
*Direct attributes
*Data stream attributes
*Related attributes

Cumulus Financial wants its service agents to view a display of all cases associated with a Unified Individual on a contact record. Which two features should a consultant consider for this use case? Choose 2 answers
Query API
*Lightning Web Components
Data Action
*Profile API
Explanation: Lightning Web Components: These can be utilized to build customized user interfaces within Salesforce. By leveraging Lightning Web Components, the consultant can create a tailored display on the contact record that showcases all cases associated with the UnifiedIndividual. This provides a user-friendly and integrated way for service agents to view this information within the Salesforce environment. Profile API: The Profile API can be employed to retrieve specific information associated with an individual or a profile. In this scenario, it could be used to fetch and display all cases linked to the UnifiedIndividual on the contact record, allowing service agents to access this information seamlessly without switching between different interfaces or systems. Utilizing Lightning Web Components for the user interface and Profile API to access and display case information can offer an integrated and efficient solution for service agents to view cases associated with UnifiedIndividuals directly from the contact record within Salesforce.

Which two common use cases can be addressed with Data Cloud? Choose 2 answers.
*Understand and act upon customer data to drive more relevant experiences.
*Harmonize data from multiple sources with a standardized and extendable data model.
Safeguard critical business data by serving as a centralized system for backup and disaster recovery.
Govern enterprise data lifecycle through a centralized set of policies and processes.

A consultant is building a segment to announce a new product launch for customers that have previously purchased black pants. How should the consultant place the attributes for product color and product type from the Order Product object to meet this criteria?
*Place the attribute for product color in one container and the attribute for product type in another container.
Place the attributes for product color and product type in a single container.
Place the attributes for product color and product type as direct attributes.
Place an attribute for the "black pants" calculated insight to dynamically apply.
Explanation: This approach allows the consultant to differentiate between the specific attributes of the product (color and type) by placing them in separate containers. By organizing attributes into distinct containers, it enables precise selection criteria for the segment—customers who purchased a product with the color attribute "black" in one container and the type attribute "pants" in another container. This segregation allows for the creation of a precise segment based on the combined criteria of color and product type, ensuring that customers who have purchased black pants are accurately targeted for the announcement of the new product launch.

A consultant is discussing the benefits of Data Cloud with a customer that has multiple disjointed data sources. Which two functional areas should the consultant highlight in relation to managing customer data? Choose 2 answers
*Data Harmonization
Data Market place
*Unified Profiles
Master Data Management
Explanation: Data Harmonization ensures that disparate data can be standardized and made compatible, while Unified Profiles help in creating a singular, comprehensive view of customers from various sources.

A consultant needs to package Data Cloud components from one organization to another. Which two Data Cloud components should the consultant include in a data kit to achieve this goal? Choose 2 answers
*Calculated insights
Identity resolution rule sets
Segments
*Data model objects
Explanation: Data Model Objects encapsulate the structure and organization of data within Data Cloud. When moving components between organizations, including these objects ensures the integrity and representation of the data's structure remain intact. Calculated Insights, on the other hand, represent the analytical outputs derived from the data. Including these insights in the data kit allows for the transfer of valuable analytics and derived information, ensuring that the previous organization's insights are retained in the new environment.

What is the result of a segmentation criteria filtering on City |IsEqual To| 'San José'?
*Cities only containing 'San José' or 'sanjosé'
Cities containing 'San José', 'San Jose', 'sanjose', or 'sanjose'
Cities only containing 'San Jose' or 'San Jose'
Cities only containing 'San Jose' or 'sanjose'
Explanation: This is because the operation <code><strong>Is Equal To</strong></code> in this context implies an exact match, considering the case sensitivity. Therefore, it will only include cities that exactly match 'SanJosé' or 'sanjosé' and exclude variations like 'SanJose', 'SanJose', or 'sanjose'.

How does Data Cloud handle an individual's Right to be Forgotten?
Deletes the records from all data source objects, and any downstream data model objects are updated at the next scheduled ingestion.
Deletes the specified Individual and records from any data source object mapped to the Individual data model object.
*Deletes the specified Individual and records from any data model object/data lake object related to the Individual.
Deletes the specified Individual record and its Unified Individual Link record.
Explanation: Data Cloud handles the Right to be Forgotten by removing the specified individual and their related records directly from the data model objects or data lake objects associated with that individual. This process ensures the erasure of their data from the designated storage within Data Cloud.

Cumulus Financial wants to be able to track the daily transaction volume of each of its customers in real time and send out a notification as soon as it detects volume outside a customer's normal range. What should a consultant do to accommodate this request?
Use a calculated insight paired with a flow.
Use streaming data transform with a flow.
*Use streaming data transform combined with a data action.
Use a streaming insight paired with a data action.
Explanation: By using streaming data transform in tandem with a data action, Cumulus Financial can achieve real-time monitoring of transaction volumes and receive timely notifications when volumes deviate from a customer's normal range. This combination ensures swift detection and response to abnormal transaction patterns.

During an implementation project, a consultant completed ingestion of all data streams for their customer. Prior to segmenting and acting on that data, which additional configuration is required?
*Identity Resolution
Calculated Insights
Data Activation
Data Mapping

A consultant is planning the ingestion of a data stream that has profile information including a mobile phone number. To ensure that the phone number can be used for future SMS campaigns, they need to confirm the phone number field is in the proper E164 Phone Number format. However, the phone numbers in the file appear to be in varying formats. What is the most efficient way to guarantee that the various phone number formats arestandardized?
Assign the Phone Number field type when creating the data stream.
Create a formula field to standardize the format.
*Edit and update the data in the source system prior to sending to Data Cloud.
Create a calculated insight after ingestion.
Explanation: Standardizing the phone number formats at the source system ensures that the data ingested into DataCloud already complies with the required E.164PhoneNumber format. By modifying the data before it reaches the ingestion point, you prevent the need for additional processing, transformation, or conversion within DataCloud itself, thereby improving efficiency and accuracy. This approach minimizes the complexity of handling varied formats within DataCloud and streamlines the ingestion process by ensuring that the phone numbers conform to the specified format from the outset.

A Data Cloud consultant is in the process of setting up data streams for a new service-based data source. When ingesting Case data, which field is recommended to be associated with the Event Time field?
*Creation Date
Escalation Date
Last Modified Date
Resolution Date

A consultant is helping a beauty company ingest its profile data into Data Cloud. The company's source data includes several fields, such as eye color, skin type, and hair color, that are not fields in the standard Individual data model object (DMO). What should the consultant recommend to map this data to be used for both segmentation and identity resolution?
Duplicate the standard Individual DMO and add the additional fields.
*Create custom fields on the standard Individual DMO.
Create a custom DMO from scratch that has all fields that are needed.
Create a custom DMO with only the additional fields and map it to the standard Individual DMO.

Which data model subject area should be used for any Organization, Individual, or Member in the Customer 360 data model?
Engagement
Global Account
Membership
*Party
Explanation: The "Party" subject area typically covers a wide range of data related to individuals, organizations, or any entity that interacts with the organization. It includes demographic information, contact details, preferences, and other relevant information about the customer or member. This subject area forms the foundational element in many Customer360 models as it captures the basic details of the involved parties.

During discovery, which feature should a consultant highlight for a customer who has multiple data sources and needs to match and reconcile data about individuals into a single unified profile?
Data Cleansing
Harmonization
Data Consolidation
*Identity Resolution
Explanation: This feature involves the process of matching and consolidating data from various sources to identify and link records belonging to the same individual. It focuses on accurately determining that multiple records across datasets indeed represent the same person. This process is fundamental in creating a unified profile by resolving and consolidating disparate data into a single cohesive representation for each individual. Identity Resolution helps in removing duplicates, reconciling conflicting information, and creating a comprehensive view of an individual across various data sources. Therefore, for a customer dealing with multiple data sources and aiming to create unified profiles, Identity Resolution becomes a crucial feature to highlight during discovery.

Cumulus Financial uses Service Cloud as its CRM and stores mobile phone, home phone, and work phone as three separate fields for its customers on the Contact record. The company plans to use Data Cloud and ingest the Contact object via the CRM Connector. What is the most efficient approach that a consultant should take when ingesting this data to ensure all the different phone numbers are properly mapped and available for use in activation?
Ingest the Contact object and then create a calculated insight to normalize the phone numbers, and then map to the Contact Point Phone data map object.
Ingest the Contact object and map the Work Phone, Mobile Phone, and Home Phone to the Contact Point Phone data map object from the Contact data stream.
Ingest the Contact object and create formula fields in the Contact datastream on the phone numbers, and then map to the Contact Point Phone data map object.
*Ingest the Contact object and use streaming transforms to normalize the phone numbers from the Contact datastream into a separate Phone data lake object (DLO) that contains three rows, and then map this new DLO to the Contact Point Phone data map object.

A customer has outlined requirements to trigger a journey for an abandoned browse behavior. Based on the requirements, the consultant determines they will use streaming insights to trigger a data action to Journey Builder every hour. How should the consultant configure the solution to ensure the data action is triggered at the cadence required?
Set the journey entry schedule to run every hour.
*Set the insights aggregation time window to 1 hour.
Set the activation schedule to hourly.
Configure the data to be ingested in hourly batches.

A consultant is integrating an Amazon S3 activated campaign with the customer's destination system. In order for the destination system to find the metadata about the segment, which file on the S3 will contain this information for processing?
The .csv file
*The json file
The .txt file
The .zip file
Explanation: JSON (JavaScript Object Notation) files are commonly used for structured data storage and transmission. In the context of an Amazon S3 activated campaign, the JSON file format is often utilized to contain metadata and structured information about segments, configurations, or settings related to the campaign. JSON files offer a structured and readable format that can encapsulate various metadata details, making it convenient for systems to process and extract necessary information about segments and their attributes. This file format is suitable for storing and conveying metadata in a structured and organized manner, often used for configurations and data descriptions.

A user has built a segment in Data Cloud and is in the process of creating an activation. When selecting related attributes, they cannot find a specific set of attributes they know to be related to the individual. Which statement explains why these attributes are not available?
Activations can only include 1-to-1 attributes.
The segment is not segmenting on profile data.
The attributes are being used in another activation.
*The desired attributes reside on different related paths.
Explanation: <strong>Attributes residing on different related paths:</strong> In Data Cloud, related attributes are accessed based on paths or connections between different attributes. If the desired attributes are located on separate or different paths, they might not be available for selection when building an activation related to a specific segment. These related paths or connections determine how attributes are linked or associated within the system. If the desired attributes are not on the same or directly linked paths as the segment being used for activation, they might not be accessible or available for selection during the activation setup. This situation often arises when the attributes needed for activation are not directly connected or part of the same linked chain of related attributes as the segment being used, leading to the unavailability of these attributes during the activation configuration.

A client wants to bring in loyalty data from a custom object in Salesforce CRM that contains a point balance for accrued hotel points and airline points within the same record. The client wantstosplitthesepointsystemsintotwoseparaterecordsforbettertrackingand processing. What should a consultant recommend in this scenario?
*Create a junction object in Salesforce CRM and modify the ingestion strategy.
Clone the data source object.
Use batch transforms to create a second data lake object.
Create a data kit from the data lake object and deploy it to the same Data- Cloud org.
Explanation: In this scenario, the consultant should recommend: C. <strong>Create a junction object in Salesforce CRM and modify the ingestion strategy.</strong> By creating a junction object in Salesforce CRM, it becomes possible to split the loyalty data into separate records, allowing for better tracking and processing of hotel points and airline points independently within their respective records. Modifying the ingestion strategy alongside this creation will ensure that the data is processed accurately, reflecting the split nature of the loyalty points systems. This approach maintains the integrity of the data within Salesforce CRM and aligns with the client's requirement for distinct records for hotel and airline points.

Which configuration supports separate Amazon S3 buckets for data ingestion and activation?
Dedicated S3 datasources in Data Cloud setup
Dedicated S3 datasources in activation setup
*Separate user credentials for datastream and activation target
Multiple S3 connectors in Data Cloud setup
Explanation: Assigning separate user credentials for the data stream (ingestion) and activation target ensures that distinct permissions and access control are established. This separation allows for different authentication or permission levels for the data being ingested (stored in Amazon S3 buckets) and the data being utilized for activation. This separation of user credentials ensures security and control over which data can be accessed for ingestion and which data is made available for activation, potentially residing in separate Amazon S3 buckets. It doesn't necessarily create separate buckets but manages access to these buckets based on distinct user credentials, maintaining security and isolation between data ingestion and activation processes.

Northern Trail Outfitters uses B2C Commerce and is exploring implementing Data Cloud to get a unified view of its customers and all their order transactions. What should the consultant keep in mind with regard to historical data when ingesting order data using the B2C Commerce Order Bundle?
The B2C Commerce Order Bundle ingests 6 months of historical data.
The B2C Commerce Order Bundle ingests 12 months of historical data.
*The B2C Commerce Order Bundle does not ingest any historical data and only ingests new orders from that point on.
The B2C Commerce Order Bundle ingests 30 days of historical data.
Explanation: Typically, with such ingestion mechanisms, the process is set up to begin capturing and ingesting data from the moment it's implemented or activated. It doesn't backfill historical data; instead, it captures and processes new orders or transactions moving forward. Therefore, for historical data analysis or the creation of a comprehensive view including past transactions, alternative methods or data sources might be needed to integrate or import historical order data into Data Cloud separately. <br>

A Data Cloud consultant recently discovered that their identity resolution process is matching individuals that share email addresses or phone numbers, but are not actually the same individual. What should the consultant do to address this issue?
*Create and run a new ruleset with stricter matching criteria, compare the two rulesets to review and verify the results, and then migrate to the new ruleset once approved.
Modify the existing ruleset with stricter matching criteria, run the ruleset and review the updated results, then adjust as needed until the individuals are matching correctly.
Create and run a new ruleset with fewer matching rules, compare the two rulesets to review and verify the results, and then migrate to the new ruleset once approved. <br>
Modify the existing ruleset to use fewer matching rules, run the ruleset and review the updated results, then adjust as needed until the individuals are matching correctly. <br>
Explanation: Creating a new ruleset with stricter matching criteria allows for a clear separation between the current rules and the refined ones. By comparing the results of both rulesets, the consultant can directly assess the effectiveness of the changes made. If the new ruleset with stricter criteria consistently produces more accurate matches without excessive false positives, it can then be migrated to replace the existing ruleset.

To import campaign members into a campaign in Salesforce CRM, a user wants to export the segment to Amazon S3. The resulting file needs to include the Salesforce CRM Campaign ID in the name. What are two ways to achieve this outcome? Choose 2 answers
*Include campaign identifier in the file name specification.
Include campaign identifier in the segment name.
Hard code the campaign identifier as a new attribute in the campaign activation.
*Include campaign identifier in the activation name.
Explanation: Include campaign identifier in the activation name: This option implies naming the activation file in a way that it includes the campaign identifier. For instance, the filename could incorporate the Salesforce CRM Campaign ID to differentiate the exported segment. Include campaign identifier in the filename specification: This suggests having a naming convention or specification for the exported file that includes the campaign identifier within the filename itself. This way, when exporting the segment to Amazon S3, the filename will automatically contain the Salesforce CRM Campaign ID. Both options propose including the campaign identifier in the exported file's name or specification, allowing for easy identification and association of the exported segment with the Salesforce CRM Campaign ID.

Northern Trail Outfitters (NTO) creates a calculated insight to compute recency, frequency, monetary (RFM) scores on its unified individuals. NTO then creates a segment based on these scores that it activates to a Marketing Cloud activation target. Which two actions are required when configuring the activation? Choose 2 answers
*Choose a segment.
Add the calculated insight in the activation.
Add additional attributes.
*Select contact points.

During a privacy law discussion with a customer, the customer indicates they need to honor requests for the right to be forgotten. The consultant determines that Consent API will solve this business need. Which two considerations should the consultant inform the customer about? Choose 2 answers
Data deletion requests are reprocessed at 30, 60, and 90 days.
Data deletion requests are processed within 1 hour.
*Data deletion requests submitted to Data Cloud are passed to all connected Salesforce clouds.
*Data deletion requests are submitted for Individual profiles.
Explanation: Data deletion requests are submitted for Individual profiles: The Consent API typically processes data deletion requests at an individual profile level. This means that when a request for the right to be forgotten is made, it pertains specifically to the individual's profile or data record within the system. Data deletion requests submitted to DataCloud are passed to all connected Salesforce clouds: The Salesforce clouds connected to DataCloud, such as Sales Cloud, Service Cloud, Marketing Cloud, etc., are part of the Salesforce ecosystem. When a data deletion request is submitted through the Consent API in DataCloud, it triggers a cascade effect where the request is transmitted to all connected Salesforce clouds, ensuring that the individual's data is deleted consistently across the entire Salesforce environment.

How does identity resolution select attributes for unified individuals when there is conflicting information in the data model?
Creates additional rulesets
*Leverages reconciliation rules
Leverages match rules
Creates additional contact points
Explanation: Reconciliation rules are specifically designed to resolve conflicts or discrepancies between different data sources or conflicting information within the same data model. These rules help determine which pieces of information should be prioritized, merged, or resolved when creating unified profiles for individuals. Reconciliation rules establish criteria or guidelines for prioritizing data sources, determining the most accurate or recent information, and merging conflicting attributes to create a coherent and accurate representation of an individual within the system. This process helps ensure that the unified individual profiles generated are as accurate and complete as possible despite conflicting data.

Northern Trail Outfitters wants to use some of its Marketing Cloud data in Data Cloud. Which engagement channel data will require custom integration?
SMS
Email
*Mobile push
Cloud Page
Explanation: Integrating Mobile Push data from Marketing Cloud into Data Cloud often requires custom integration due to the specialized nature of this channel. Mobile push notifications involve unique identifiers, specific engagement metrics, and potentially different data structures compared to more traditional channels like Email or SMS. This custom integration may involve mapping the data fields, ensuring the correct handling of mobile device identifiers, capturing engagement metrics unique to push notifications, and aligning these details with the structure of Data Cloud for seamless data ingestion and analysis.

A retail customer wants to bring customer data from different sources and wants to take advantage of identity resolution so that it can be used in segmentation.
Subscriber
Unified Contact
*Unified Individual
Individual
Explanation: The "UnifiedIndividual" entity typically represents a consolidated and unified view of a customer across different data sources, thanks to identity resolution processes. This entity aggregates and reconciles data from various sources to create a comprehensive profile, making it suitable for segmentation aimed at activating memberships.

A healthcare client wants to make use of identity resolution, but does not want to risk unifying profiles that may share certain personally identifying information (PII). Which matching rule criteria should a consultant recommend for the most accurate matching results?
Email Address and Phone
Fuzzy First Name, Exact Last Name, and Email
Exact Last Name and Email
*Party Identification on Patient ID
Explanation: In healthcare settings, the Patient ID is a unique identifier assigned to each patient, allowing for the accurate identification of individuals without using directly sensitive information like names or contact details. Utilizing a Party Identification based on Patient ID ensures accurate matching while minimizing the risk of merging profiles with shared PII.

Northern Trail Outfitters isusing the Marketing Cloud Starter Data Bundles to bring Marketing Cloud data into Data Cloud. What are two of the available data sets in Marketing Cloud Starter Data Bundles? Choose 2 answers
*Mobile Connect
Personalization
*Mobile Push
Loyalty Management

Cloud Kicks wants to be able to build a segment of customers who have visited its website within the previous 7 days. Which filter operator on the Engagement Date field fits this use case?
Next Number of Days
*Last Number of Days
Greater than Last Number of Days
Is Between
Explanation: The "Last Number of Days" filter operator would be appropriate in this scenario because it allows you to filter data based on a time frame relative to the current date. Specifically, selecting "Last 7 Days" would help identify customers who engaged with the website within the last week. This operator is suitable for situations where you need to specify a time range relative to the present moment rather than using specific dates.Which consideration related to the way Data Cloud ingests CRM data is true?
*The CRM Connector allows standard fields to stream into Data Cloud in real time.
Formula fields are refreshed at regular sync intervals and are updated at the next full refresh.
The CRM Connector's synchronization times can be customized to up to 15-minute intervals.
CRM data cannot be manually refreshed and must wait for the next scheduled synchronization.
Explanation: within Data Cloud, the CRM Connector enables real-time streaming of standard fields from the CRM system. This means that changes made in these standard fields within the CRM are immediately synchronized and reflected in the Data Cloud environment, ensuring that the data stays current and accurate for analysis and other purposes.

Which two dependencies prevent a data stream from being deleted? Choose 2 answers
The underlying data lake object is used in a data transform.
The underlying data lake object is used in activation.
*The underlying data lake object is mapped to a data model object.
*The underlying data lake object is used in segmentation.
Explanation: When a data lake object is mapped to a data model object, a dependency is established. Deleting the data stream becomes restricted as it's linked to a specific data model object. Removing the data stream would potentially disrupt the flow or mapping between the data lake object and the data model, impacting downstream processes relying on this mapping. If the underlying data lake object is utilized in segmentation, attempting to delete the data stream becomes restricted. Segmentation uses this object as a reference or source, and deleting the data stream could disrupt or invalidate the segmentation processes reliant on that particular object.

Which two requirements must be met for a calculated insight to appear in the segmentation canvas? Choose 2 answers
* The primary key of the segmented table must be a dimension in the calculated insight.
The primary key of the segmented table must be a metric in the calculated insight.
The metrics of the calculated insights must only contain numeric values.
* The calculated insight must contain a dimension including the Individual or Unified Individual Id.
Explanation: <strong>The primary key of the segmented table must be a dimension in the calculated insight :</strong> In segmentation, dimensions are attributes used to categorize and filter data. For a calculated insight to appear in the segmentation canvas, the primary key (which is essential for linking and identifying records in the segmented table) needs to be part of the dimensions within the calculated insight. <strong>The calculated insight must contain a dimension including the Individual or Unified Individual Id :</strong> The Individual or Unified Individual Id is crucial for linking and identifying specific individuals within the segmented data. Including this identifier as a dimension in the calculated insight ensures that the insight provides data at an individual level, allowing for personalized segmentation based on unique identities.

Where is value suggestion for attributes in seqmentation enabled when creating the DMO?
Data Mapping
*Data Transformation
Data Stream Setup
Segment Setup
Explanation: Value suggestion for attributes typically occurs within the context of data transformation processes. When setting up a Data Model Object (DMO) within Data Transformation, you can configure and define attributes, specifying their characteristics and properties. This setup often includes enabling features like value suggestion, which provides recommendations or suggestions for attribute values based on the existing data or predefined parameters. This functionality aids in data consistency, accuracy, and efficiency during segmentation or other data-related operations.

A customer has a requirement to receive a notification whenever an activation fails for a particular segment. Which feature should the consultant use to solution for this use case?
Dashboard
Report
*Activation alert
Flow
Explanation: Activation alerts are specifically designed to notify users or stakeholders about specific events or occurrences within the activation process. In this scenario, setting up an activation alert for the particular segment would enable the customer to receive notifications whenever there's a failure in activating that specific segment. This feature allows for proactive monitoring and immediate awareness of any issues related to segment activation.

What should a user do to pause a segment activation with the intent of using that segment again?
Delete the segment.
Stop the publish schedule.
Skip the activation.
*Deactivate the segment.
Explanation: Deactivating the segment temporarily halts its activation while preserving its settings and configurations. This action allows the user to pause the segment's current activation schedule or activities without losing the segment itself. Once deactivated, the user can later reactivate the segment when needed, using it again for various purposes without having to recreate it from scratch. <br>

Northern Trail Outfitters (NTO) wants to connect their B2C Commerce data with Data Cloud and bring two years of transactional history into Data Cloud. What should NTO use to achieve this?
Direct Sales Product entity ingestion
B2C Commerce Starter Bundles
Direct Sales Order entity ingestion
*B2C Commerce Starter Bundles plus a custom extract
Explanation: Northern Trail Outfitters (NTO) should utilize a combination of the B2C Commerce Starter Bundles, which offer predefined connectors for integrating B2C Commerce data into Data Cloud, and a custom extract to fulfill their requirement of bringing two years of transactional history into Data Cloud. While the Starter Bundles provide a foundational framework, a custom extract allows NTO to extract and ingest the specific historical transactional data they need beyond what the default integration might offer. This combination ensures a comprehensive integration that meets NTO's specific historical data needs.

A client wants to bring in loyalty data from a custom object in Salesforce CRM that contains a point balance for accrued hotel points and airline points within the same record, The client wants to split these point systems into two separate records for better tracking and processing. What should 2 consultant recommend in this scenario?
Clone the data source object.
Create 8 data kit from the date lake object and deploy it to the same Data Cloud org.
Use batch transforms to create a second data lake object.
*Create a junction object in Salesforce CRM and modify the ingestion strategy.
Explanation: By creating a junction object in Salesforce CRM, the client can establish a separate entity to handle the two types of loyalty points (hotel points and airline points). Modifying the ingestion strategy to incorporate this junction object allows for a more structured and organized way to manage and process the loyalty point data separately.

Cumulus Financial is currently using Data Cloud and ingesting transactional data from its backend system via an S3 Connector in upsert mode. During the initial setup six months ago, the company created a formula field in Data Cloud to create a custom classification. It now needs to update this formula to account for more classifications. What should the consultant keep in mind with regard to formula field updates when using the S3 Connector?
Data Cloud will only update the formula on a go-forward basis for new records.
Data Cloud will initiate a full refresh of data from $3 and will update the formula on all records.
*Data Cloud will update the formula for all records at the next incremental upsert refresh.
Data Cloud does not support formula field updates for data streams of type upsert.
Explanation: When using the S3 Connector in upsert mode, updating a formula field in Data Cloud does not trigger an immediate full refresh of data from the S3 source. Instead, during the next incremental upsert refresh, the updated formula will be applied to all records, ensuring consistency across the dataset. This process allows for updates to be reflected in the existing records as part of the regular data synchronization without necessitating a complete re-import or refresh from the S3 source.

A customer is trying to activate data from Data Cloud to an Amazon S3 Cloud File Storage Bucket. Which authentication type should the consultant recommend to connect to the S3 bucket from Data Cloud?
*Use an S3 Access Key and Secret Key.
Use an S3 Private Key Certificate.
Use a JWT Token generated on S3.
Use an S3 Encrypted Username and Password.
Explanation: This method involves providing an Access Key ID and Secret Access Key generated from the AWS Identity and Access Management (IAM) service. These keys are used to securely authenticate and access resources within the S3 bucket. It's a standard and widely accepted way to authenticate access to AWS services like S3.

A customer has a Master Customer table from their CRM to ingest into Data Cloud. The table contains a name and primary email address, along with other personally Identifiable information (PII). How should the fields be mapped to support identity resolution?
Create a new custom object with fields that directly match the incoming table.
Map all fields to the Customer object.
*Map name to the Individual object and email address to the Contact Phone Email object.
Map all fields to the Individual object, adding a custom field for the email address.
Explanation: Mapping the fields from the CRM's Master Customer table to facilitate identity resolution involves directing the name field to the Individual object and the primary email address to the Contact Phone Email object in Data Cloud. This setup ensures that individual-specific data like names aligns with the Individual object, while crucial email information gets associated with the Contact Phone Email object, enabling effective identity resolution processes within the platform.

During a privacy law discussion with a customer, the customer indicates they need to honor requests for the right to be forgotten. The consultant determines that Consent APT will solve this business need. Which two considerations should the consultant inform the customer about? Choose 2 answers
Data deletion requests are processed within 1 hour.
* Data deletion requests submitted to Data Cloud are passed to all connected Salesforce clouds.
* Date deletion requests are reprocessed at 30, 60, and 90 days.
Data deletion requests are submitted for Individual profiles.
Explanation: <strong>Data Deletion Requests Passed to Connected Salesforce Clouds :</strong> The Consent API facilitates the handling of data deletion requests within the Data Cloud ecosystem. It's crucial for the customer to understand that these deletion requests are not limited to the Data Cloud alone but are extended to all connected Salesforce clouds. This ensures a comprehensive approach to honoring deletion requests across different Salesforce services or platforms where the customer's data might reside. <strong>Reprocessing Data Deletion Requests at Intervals:</strong> The customer needs to be aware that data deletion requests might not take immediate effect. The reprocessing of deletion requests at specific intervals (30, 60, and 90 days) implies that the data deletion process is scheduled periodically rather than being a one-time immediate deletion. This is an important consideration for managing expectations regarding the timing and frequency of data deletion after the right to be forgotten request is submitted.

Where is value suggestion for attributes in segmentation enabled when creating the DMO?
*Data Mapping
Data Stream Setup
Data Transformation
Segment Setup
Explanation: Value suggestion for attributes in segmentation, which assists in suggesting attribute values during segment creation, is enabled within the Segment Setup phase in a Data Management Organization (DMO). This feature streamlines the process by suggesting relevant attribute values based on available data, making it easier for users to define segment criteria accurately.

Northern Trail Outfitters (NTO) wants to send a promotional campaign for customers that have purchased within the past 6 months. The consultant created a segment to meet this requirement. Now, NTO brings an additional requirement to suppress customers who have made purchases within the last week. What should the consultant use to remove the recent customers?
Related attributes
*Segmentation exclude rules
Streaming insights
Batch transforms
Explanation: Segmentation exclude rules to remove customers who have made purchases within the last week. This functionality allows for the refinement or exclusion of certain segments based on specific criteria. Segmentation exclude rules enable the consultant to further filter or refine the segment created for customers who purchased within the past 6 months by excluding those who made purchases within the last week. This way, the promotional campaign will target customers who made purchases within the specified time frame (past 6 months) while excluding the most recent ones.

Which two dependencies prevent a data stream from being deleted? Choose 2 answers
*The underlying data lake object is used in segmentation.
*The underlying data lake object is mapped to a data model object.
The underlying data lake object is used in activation.
The underlying data lake object is used in a data transform.
Explanation: <strong>The underlying data lake object is mapped to a data model object:</strong> If the data lake object is directly mapped to a data model object, deleting the data stream might disrupt the functionality or integrity of the connected data model, leading to dependencies that prevent its deletion. <strong>The underlying data lake object is used in segmentation:</strong> When a data lake object is actively utilized in segmentation processes, deleting it can affect the segmentation functionalities or workflows that rely on this data stream. The dependencies in segmentation prevent the deletion of the data stream.

A customer needs to integrate in real time with Salesforce CRM. Which feature accomplishes this requirement?
Data actions and Lightning web components
*Data model triggers
Streaming transforms
Sales and Service bundle
Explanation: "Data model triggers" are not explicitly a feature in Salesforce but could refer to triggers in the Salesforce data model. Triggers in Salesforce are Apex code that runs before or after specific data manipulation language (DML) events occur, such as insertions, updates, or deletions of records. They are used to perform custom actions or logic in response to these events.

Northern Trail Outfitters (NTO), an outdoor lifestyle clothing brand, recently started a new line of business. The new business specializes in gourmet camping food. For business reasons as well as security reasons, it's important to NTO to keep all Data Cloud data separated by brand. Which capability best supports NTO's desire to separate its data by brand?
Data sources for each brand
Data spaces for each brand
Data model objects for each brand
*Data streams for each brand
Explanation: Using data streams for each brand allows Northern Trail Outfitters to separate and manage its data effectively within Data Cloud. This approach involves creating distinct data streams for different brands or business lines, enabling the organization to organize and handle data separately for its various ventures, like the outdoor clothing brand and the new gourmet camping food business. This segregation ensures that data remains distinct and manageable independently within Data Cloud, aligning with NTO's need to keep its data separated by brand for both business and security reasons.

An organization wants to enable users with the ability to identify and select text attributes from a picklist of options. Which Data Cloud feature should help with this use case?
*Global picklists
Data harmonization
Value suggestion
Transformation formulas
Explanation: Global picklists in Data Cloud provide a way to create standardized lists of values that users can select from when entering text attributes. They offer a predefined set of options for users to choose from, ensuring consistency and accuracy in data entry by limiting selections to the provided list.

Northern Trail Outtitters uploads new customer data to an Amazon S3 Bucket on a daily basis to be ingested in Data Cloud.
Refresh Data Stream &gt; Calculated Insight &gt; Identity Resolution
Identity Resolution &gt; Refresh Data Stream &gt; Calculated Insight
Calculated Insight &gt; Refresh Data Stream &gt; Identity Resolution
*Refresh Data Stream &gt; Identity Resolution &gt; Calculated Insight
Explanation: <strong>Refresh Data Stream:</strong> Initiating the data stream refresh ensures that the latest data from the Amazon S3 Bucket is brought into Data Cloud. This step ensures that the system has the most recent data available for processing.</p></li><li><p><strong>Identity Resolution:</strong> Following the data refresh, performing identity resolution is crucial. This process helps reconcile and link data from various sources, establishing a unified view of customers/entities across datasets. Ensuring data consistency and accuracy through identity resolution is fundamental before utilizing the data for segmentation or insights.</p></li><li><p><strong>Calculated Insight:</strong> Once the data is refreshed and identity resolution is completed, running calculated insights can utilize the unified and updated dataset. Calculated insights can extract meaningful information or analytics from the data, aiding in segmentation or other data-driven actions.</p></li></ol><p>This sequence ensures a structured approach where the data is refreshed first, followed by necessary reconciliation and then utilizing the updated dataset for insights or segmentation purposes.

A retailer wants to unify profiles using Loyalty ID which is different than the unique ID of their customers. Which object should the consultant use in identity resolution to perform exact match rules on the Loyalty ID?
*Party Identification object
Individual object
Loyalty Identification object
Contact Identification object
Explanation: The Party Identification object within identity resolution is designed to manage and resolve identities across various identifiers. When a retailer aims to unify profiles using a Loyalty ID that differs from the unique customer ID, using the Party Identification object allows for the creation of precise rules for exact match resolutions based specifically on Loyalty IDs. This object is tailored to handle diverse identifiers, ensuring accurate matching criteria for unifying profiles based on the Loyalty ID as a specific identifier. <br>

Northern Trail Outfitters (NTO) creates a calculated insight to compute recency, frequency, monetary (RFM) scores on its unified individuals. NTO then creates a segment based on these scores that it activates to a Marketing Cloud activation target. Which two actions are required when configuring the activation?
Add additional attributes.
Add the calculated insight in the activation.
*Select contact points.
*Choose a segment.
Explanation: Choose a segment: This is a necessary step in configuring activation. Once the RFM scores are calculated and segments are created based on these scores, selecting the specific segment or segments that NTO wants to activate is crucial. Therefore, choosing a segment is an essential action.  Select contact points: In the context of activation, selecting contact points is also crucial. Contact points refer to the channels or touch points through which NTO intends to reach its audience in the Marketing Cloud. This could include email, social media, SMS, etc. Selecting the appropriate contact points is essential to ensure the targeted segments receive the intended marketing communications.

A consultant wants to ensure that every segment managed by multiple brand teams adheres to the same set of exclusion criteria, that are updated on a monthly basis. What is the most efficient option to allow for this capability?
Create a segment and copy it for each brand.
*Create a reusable container block with common criteria.
Create a nested segment.
Create, publish, and deploy a data kit.
Explanation: By using a reusable container block, the consultant can create a standardized set of exclusion criteria that can be shared across multiple segments managed by different brand teams. Any updates or changes to the exclusion criteria need to be done only once within the container block, ensuring consistency across all segments that use it. This helps in maintaining uniformity and streamlining the management of exclusion criteria, making it efficient especially when updates are required on a monthly basis.

A user wants to be able to create a multi-dimensional metric to identify unified individual lifetime value (LTV). Which sequence of data model object (DMO) joins is necessary within the calculated Insight to enable this calculation?
Sales Order &gt; Individual &gt; Unified Individual
Unified Individual &gt; Individual &gt; Sales Order
*Unified Individual &gt; Unified Link Individual &gt; Sales Order
Sales Order &gt; Unified Individual
Explanation: By joining these Data Model Objects in this sequence, you connect the Unified Individuals to their associated Sales Orders through an intermediary link, allowing you to calculate a comprehensive, multi-dimensional metric for unified individual lifetime value by considering information from both Unified Individuals and their corresponding Sales Orders. <br>

Customer has a master customer table from their CRM to injust into data cloud the table contains a name and primary email address along with other personally identifyable information PIL how should the the fields be mad to support identity resolution?
map all fields to the individual object adding a custom field to the email address
create a new custom object with field the directly measure the incoming table
map all fields to the customer object 
*map name to the individual object and email address to the contact phone email object
Explanation: By mapping the name to the Individual Object and the email address to the Contact Point Email Object, the Data Cloud environment can effectively resolve and link individuals across datasets based on these key identifiers, enhancing the accuracy and reliability of identity resolution processes.

During an implementation project, a consultant completed ingestion of all data streams for their customer.
Data Activation
Calculated Insights
*Identity Resolution
Data Mapping
Explanation: Before moving on to segmenting and acting on the ingested data, setting up Identity Resolution is crucial. Identity Resolution helps reconcile and link data from various sources that refer to the same individual or entity, creating a unified view. This process ensures that the data is associated correctly, eliminating duplicates and inconsistencies that might arise from multiple sources. Having a unified and accurate view of individuals/entities is fundamental for effective segmentation and subsequent actions on the data.

A customer has a requirement to receive a notification whenever an activation fails for a particular segment. Which feature should the consultant use to solution for this use case?
*Flow
Dashboard
Activation alert
Report
Explanation: Flows in Data Cloud enable the automation of processes based on specific triggers or events. In this case, a consultant can set up a Flow to monitor segment activations and trigger a notification whenever an activation fails for the specified segment. Flows offer customizable, automated actions in response to defined conditions, making them the ideal choice for creating notifications in the event of activation failures.

Cumulus Financial created a segment called Multiple Investments that contains individuals who have invested in two or more mutual funds. The company plans to send an email to this segment regarding a new mutual fund offering, and wants to personalize the email content with information about each customer's current mutual fund investments. How should the Data Cloud consultant configure this activation?
Include Fund Type equal to "Mutual Fund" as a related attribute. Configure an activation based on the new segment with no additional attributes.
*Choose the Multiple Investments segment, choose the Email contact point, add related attribute Fund Name, and add related attribute filter for Fund Type equal to "Mutual Fund".
Choose the Multiple Investments segment, choose the Email contact point, and add related attribute Fund Type.
Include Fund Name and Fund Type by default for post processing in the target system.
Explanation: This configuration ensures that the email content is personalized based on each customer's current mutual fund investments, allowing for targeted communication about the new mutual fund offering to individuals who have already invested in mutual funds.

When creating a segment on an individual, what is the result of using two separate containers linked by an AND as shown below? GoodsProduct | Count | At Least | 1 Color | Is Equal To I red AND GoodsProduct | Count | At Least | 1 PrimaryProductCategory | Is Equal To I shoes
Individuals who purchased at least one 'red shoes' as a single line item in a purchase
Individuals who made a purchase of at least one 'red shoes' and nothing else
Individuals who purchased at least one of any 'red' product or purchased at least one pair of 'shoes'
*Individuals who purchased at least one of any 'red' product and also purchased at least one pair of 'shoes'
Explanation: using two separate containers linked by an AND operator in a segment definition means that both conditions must be met simultaneously for an individual to be included in that segment. In this case, it identifies individuals who have purchased at least one 'red' product and, at the same time, have also bought at least one pair of 'shoes'. The AND operator implies that both conditions must be true for someone to be part of this segment.

Which data model subject area detines the revenue or quantity for an opportunity by product family?
Party
Engagement
Sales Order
* Product
Explanation: This subject area often encompasses details related to products, including categorization by product families, along with associated metrics like revenue, quantity, or other product-related data. In the context of an opportunity, the Product subject area provides insights into the products associated with opportunities, allowing for analysis and reporting based on product families, revenue, quantities, or other relevant metrics tied to those products within opportunities.

Which consideration related to the way Data Cloud ingests CRM data is true?
Formula fields are refreshed at regular sync intervals and are updated at the next full refresh.
The CRM Connector's synchronization times can be customized to up to 15-minute intervals.
*CRM data cannot be manually refreshed and must wait for the next scheduled synchronization.
The CRM Connector allows standard fields to stream into Data Cloud in real time.
Explanation: Typically, CRM data synchronized into Data Cloud cannot be manually refreshed or updated outside of the scheduled synchronization intervals. Changes or updates made in the CRM may not immediately reflect in the Data Cloud environment but will wait for the next scheduled synchronization cycle to be reflected in the system.

Cumulus Financial created a segment called High Investment Balance Customers. This is a foundational segment that includes several segmentation criteria the marketing team should consistently use. Which feature should the consultant suggest the marketing team use to ensure this consistency when creating future, more refined segments?
Package High Investment Balance Customers in a data kit.
Create a High Investment Balance calculated insight
Create new segments by cloning High Investment Balance Customers.
*Create new segments using nested segments.
Explanation: Nested segments involve creating new segments by incorporating or nesting existing segments within them. By utilizing nested segments, the marketing team can build more refined segments while maintaining consistency by including the foundational "High Investment Balance Customers" segment within these new segments. This approach ensures that the criteria from the foundational segment are consistently used as a base within more refined segments. By nesting segments, the team can add additional criteria or layers of segmentation while keeping the foundational criteria intact, ensuring consistency and alignment with the original segment. <br>

When creating a seqment on an individual, what is the result of using two separate containers linked by an AND as shown below? GoodsProduct | Count | At Least | 1 Color | Is Equal To | red AND GoodsProduct | Count | At Least | 1 PrimaryPraductCategory | Is Equal To | shoes
Individuals who made a purchase of at least one ‘red shoes’ and nothing else
Individuals who purchased at least one of any ‘red’ product and also purchased at least one pair of ‘shoes’
Individuals who purchased at least one ‘red shoes’ as a single line item in a purchase
*Individuals who purchased at least one of any ‘red’ product or purchased at least one pair of ‘shoes’
Explanation: Individuals who purchased at least one product that is both 'red' and falls into the 'shoes' category. The "AND" operator between the two conditions narrows down the segment to individuals who meet both criteria simultaneously: having purchased at least one 'red' product and also having purchased at least one product that belongs to the 'shoes' category. This combination requirement implies that the individuals must meet both conditions to be included in the segment.

Northern Trail Outfitters wants to be able to calculate each customer's lifetime value (LTV) but also create breakdowns of the revenue sourced by website, mobile app, and retail channels. What should a consultant use to address this use case in Data Cloud?
Flow Orchestration
Streaming data transform
*Metrics on metrics
Nested segments
Explanation: "Metrics on metrics" allows you to create derived metrics or calculations based on existing metrics. In this scenario, you can calculate LTV based on customer transaction history and then create separate metrics to track revenue sourced from website, mobile app, and retail channels. This method enables the creation of custom metrics that provide insights into specific aspects of customer behaviour and revenue generation across different channels, allowing Northern Trail Outfitters to gain a comprehensive understanding of their customers' lifetime value and revenue sources.

During an implementation project, a consultant completed ingestion of all data streams for their customer. Prior to segmenting and acting on that data, which additional configuration is required?
*Identity Resolution
Data Activation
Data Mapping
Calculated Insights
Explanation: Before segmenting and utilizing the ingested data, it's crucial to ensure that the data streams from different sources are linked and resolved to identify unique individuals accurately. Identity resolution involves reconciling and linking disparate data points belonging to the same individual across various datasets. This process creates a unified and accurate view of customers or entities, essential for effective segmentation and targeted actions.

A customer is concerned that the consolidation rate displayed in the identity resolution is quite low compared to their initial estimations. Which configuration change should a consultant consider in order to increase the consolidation rate?
Change reconciliation rules to most occuring.
Include additional attributes in the existing matching rules.
*Increase the number of matching rules.
Reduce the number of matching rules.
Explanation: By increasing the number of matching rules, you're essentially expanding the criteria used by the system to identify and link records belonging to the same individual across different data sources. For instance, if the existing matching rules are based on basic criteria like email address or name, adding more rules could involve considering additional data points such as phone numbers, addresses, purchase history, or behavioral patterns. This broader set of criteria allows the system to make more accurate connections between records, thus potentially increasing the consolidation rate.

How can a consultant modify attribute names to match a naming convention in Cloud File Storage targets?
Update field names in the data model object.
Use a formula field to update the field name in an activation.
Set preferred attribute names when configuring activation.
*Update attribute names in the data stream configuration.
Explanation: When dealing with Cloud File Storage targets, the attribute names can be modified to match a specific naming convention by updating the attribute names within the data stream configuration. This process allows consultants to align the attribute names as per the desired naming convention before the data is stored in the Cloud File Storage target. Adjusting attribute names at the data stream configuration level ensures that the data stored follows the prescribed naming conventions without impacting the original data source or the data model object.

Cumulus Financial uses Service Cloud as its CRM and stores mobile phone, home phone, and work phone as three separate fields for its customers on the Contact record. The company plans to use Data Cloud and ingest the Contact object via the CRM Connector. What is the most efficient approach that a consultant should take when ingesting this data to ensure all the different phone numbers are properly mapped and available for use in activation?
Ingest the Contact object and map the Work Phone, Mobile Phone, and Home Phone to the Contact Point Phone data map object from the Contact data stream.
Ingest the Contact object and create formula fields in the Contact data stream on the phone numbers, and then map to the Contact Point Phone data map object.
Ingest the Contact object and then create a calculated insight to normalize the phone numbers, and then map to the Contact Point Phone data map object.
*Ingest the Contact object and use streaming transforms to normalize the phone numbers from the Contact data stream into a separate Phone data lake object (DLO) that contains three rows, and then map this new DLO to the Contact Point Phone data map object.
Explanation: it involves ingesting the Contact object and using streaming transforms to organize and normalize the different phone numbers (mobile, home, work) into a separate structured data object called a Phone Data Lake Object (DLO). This DLO contains three rows, likely one for each type of phone number. By structuring the data this way, it becomes easier to manage and access these phone numbers for various purposes. Mapping this newly created DLO to the Contact Point Phone data map object establishes a clear connection between the specific phone numbers and the point where they will be used (activation), ensuring efficient use of the data while maintaining its integrity and organization.

A consultant is discussing the benefits of Data Cloud with a customer that has multiple disjointed data sources.Which two functional areas should the consultant highlight in relation to managing customer data? Choose 2 answers
Unified Profiles
*Data Harmonization
*Master Data Management
Data Marketplace
Explanation: <strong>Master Data Management :</strong> This involves the governance, standardization, and management of key data entities (such as customers) across various data sources. It ensures consistency, accuracy, and reliability of core data elements like customer information throughout the organization. With disjointed data sources, implementing master data management practices helps create a centralized, authoritative source for crucial customer-related data.</p></li><li><p><strong>Data Harmonization :</strong> Disjointed data sources often lead to inconsistencies in data formats, structures, and meanings. Data harmonization focuses on standardizing, integrating, and aligning disparate data into a coherent format. It involves resolving data discrepancies, ensuring uniformity, and making different data sources compatible with each other. Harmonizing data allows for easier analysis, reporting, and utilization of customer information across the organization.</p></li></ul></div></div></form></div><div class="detailed-result-panel--panel-row--4lvVX detailed-result-panel--question-container--vh1KF"><form class="mc-quiz-question--container--dV-tK"><span>

What does the Source Sequence reconciliation rule do in identity resolution?
Includes data from sources where the data is most frequently occurring
Identifies which individual records should be merged into a unified profile by setting a priority for specific data sources
Identifies which data sources should be used in the process of reconcillation by prioritizing the most recently updated data source
*Sets the priority of specific data sources when building attributes in a unified proflle, such as a first or last name
Explanation: <br> The Source Sequence reconciliation rule in identity resolution sets the priority of specific data sources when creating attributes, like first or last names, in a unified profile. It determines which data source takes precedence for certain details within the consolidated profile, ensuring accuracy and consistency in attribute selection.

Which data model subject area defines the revenue or quantity for an opportunity by product family?
Engagement
Product
Party
*Sales Order
Explanation: The Sales Order data model subject area focuses on transactions and orders, providing insights into revenue, quantity, and related details associated with sales transactions. Within this subject area, analysts can access information regarding the revenue or quantity linked to opportunities grouped by product families. It's the ideal category within the data model to explore and understand sales-related data, specifically revenue and quantities attributed to various product families within the context of opportunities.

A segment fails to refresh with the error "Segment references too many data lake objects (DLOs)". Which two troubleshooting tips should help remedy this issue? Choose 2 answers
* Refine segmentation criteria to limit up to five custom data model objects (DMOs).
* Split the segment into smaller segments.
Space out the segment schedules to reduce DLO load.
Use calculated insights in order to reduce the complexity of the segmentation query.
Explanation: <strong>Refining Segmentation Criteria :</strong> This suggests refining the criteria used for segmentation to limit the number of custom Data Model Objects (DMOs) referenced by the segment. Limiting the number of DMOs can reduce the complexity of the segmentation query, potentially alleviating the issue of referencing too many DLOs and allowing the segment to refresh successfully. <strong>Splitting the Segment :</strong> Dividing the segment into smaller segments helps reduce the workload on the system. Smaller segments mean fewer resources are required to refresh each segment, potentially resolving the issue of referencing too many DLOs and enabling successful refreshes without overwhelming the system.

How can a consultant modity attribute names to match 2 naming convention in Cloud File Storage targets?
*Update attribute names in the data strearn configuration.
Set preferred attribute names when configuring activation.
Use a formula field to update the field name in an activation.
Update field names in the data model object.
Explanation: It consultants to modify attribute names to match naming conventions in Cloud File Storage targets. By updating the field names directly within the data model object, consultants can align the attribute names to match the desired naming conventions required for Cloud File Storage targets. This ensures that when data is activated or exported to these targets, the attribute names adhere to the specified conventions, maintaining consistency and compatibility across systems.

Which two steps should a consultant take if a successfully configured Amazon S3 data stream fails to refresh with a "NO FILE FOUND" error message? Choose 2 answers
Check if correct permissions are configured for the Data Cloud user.
*Check If the file exists in the specified bucket location.
*Check if correct permissions are configured for the S3 user.
Check if the Amazon S3 data source is enabled in Data Cloud Setup.
Explanation: C. Check If the file exists in the specified bucket location: Verifying whether the file exists in the specified Amazon S3 bucket location is crucial when encountering a "NO FILE FOUND" error message during a data stream refresh. This step helps confirm if the expected file is present in the designated location. If the file is missing or located elsewhere, it would result in the failure to find the file during the refresh process. D. Check if correct permissions are configured for the S3 user: Ensuring that the appropriate permissions are set up for the Amazon S3 user is vital. The S3 user associated with the Data Cloud needs to have the necessary access and permissions to retrieve data from the specified bucket. Inadequate permissions could lead to failure in accessing or retrieving the file during the refresh process, resulting in a "NO FILE FOUND" error.

A consultant wants to ensure that every segment managed by multiple brand teams adheres to the same set of exclusion criteria, that are updated on a monthly basis. What is the most efficient option to allow for this capability?
*Create a reusable container block with common criteria.
Create a segment and copy it for each brand.
Create a nested segment.
Create, publish, and deploy a data kit.
Explanation: By creating a reusable container block containing the common exclusion criteria, the consultant can ensure consistency and efficiency. This block can encapsulate the updated exclusion criteria that need to be applied to multiple segments. When the exclusion criteria are updated on a monthly basis, modifying this container block automatically updates all segments linked to it. This approach centralizes the criteria management, ensuring uniformity across all segments without the need to replicate changes individually in each segment.

What is Data Cloud's primary value to customers?
To create a single source of truth for all anonymous data
*To provide a unified view of a customer and their related data
To create personalized campaigns by listening, understanding, and acting on customer behaviour
To connect all systems with a golden record
Explanation: Data Cloud solutions excel in aggregating, integrating, and harmonizing data from various sources. This enables the creation of a comprehensive, unified view of customers by consolidating data scattered across different platforms or systems. This unified view helps businesses understand customers better, allowing for improved decision-making, personalized interactions, and enhanced customer experiences.

A consultant has an activation that is set to publish every 12 hours, but has discovered that updates to the data prior to activation are delayed by up to 24 hours. Which two areas should a consultant review to troubleshoot this issue? Choose 2 answers
Review calculated insights to make sure they're run after the segments are refreshed.
*Review segments to ensure they're refreshed after the data is ingested.
*Review data transformations to ensure they're run after calculated insights.
Review calculated insights to make sure they're run before segments are refreshed.
Explanation: A. Review data transformations to ensure they're run after calculated insights. This option suggests reviewing the sequence of operations within the data processing pipeline. If data transformations occur before calculated insights, it might mean that insights are derived from outdated or incomplete data. Adjusting the sequence to ensure that insights are derived from the latest data can reduce delays in updates before activation. C. Review segments to ensure they're refreshed after the data is ingested. Segments need to be refreshed after new data is ingested to incorporate the latest information. If the segments are not refreshed promptly after data ingestion, they might retain outdated information, causing delays in reflecting the most recent data updates. Ensuring segments are refreshed post data ingestion helps maintain data currency and accuracy.

A segment fails to refresh with the error "Segment references too many data lake objects (DLOS)". Which two troubleshooting tips should help remedy this issue? Choose 2 answers
*Split the segment into smaller segments.
Space out the segment schedules to reduce DLO load.
*Use calculated insights in order to reduce the complexity of the segmentation query.
Refine segmentation criteria to limit up to five custom data model objects (DMOs).
Explanation: A. <strong>Split the segment into smaller segments:</strong> Breaking down the segment into smaller, more manageable segments can help reduce the complexity and the number of data lake objects referenced in each individual segment. By dividing the segment, you distribute the load across multiple segments, potentially alleviating the issue. B. <strong>Use calculated insights to reduce the complexity of the segmentation query:</strong> Calculated insights can pre-calculate certain complex computations or aggregations, reducing the complexity of the segmentation query itself. By utilizing calculated insights, you might decrease the number of data lake objects referenced or optimize the query's computational load, thus mitigating the error related to excessive DLOs. Both of these approaches aim to reduce the complexity and load associated with the segment's query or composition, thereby addressing the issue of referencing too many data lake objects and potentially resolving the segment refresh failure.

Cloud Kicks received a Request to be Forgotten by a customer, In which two ways should a consultant use Data Cloud to honour this request? Choose 2 answers
Delete the data from the incoming data stream and perform a full refresh.
Use Data Explorer to locate and manually remove the Individual.
*Use the Consent API to suppress processing and delete the individual and related records from source data streams.
*Add the Individual ID to a heacerless file and use the delete trom file functionality.
Explanation: Use the Consent API to suppress processing and delete the individual and related records from source data streams: The Consent API allows for managing consent-related requests. Using it to suppress processing and delete individual records from the source data streams ensures compliance with the request by removing the specified individual's data.  Add the Individual ID to a headerless file and use the delete from file functionality: This approach involves compiling the Individual ID into a file and using the "delete from file" functionality, likely available in Data Cloud, to facilitate the removal of the specified individual's data from the datasets.

A Data Cloud customer wants to adjust their identity resolution rules to increase their accuracy of matches. Rather than matching on email address, they want to review a rule that joins their CRM Contacts with their Marketing Contacts, where both use the CRM ID as their primary key. Which two steps should the consultant take to address this new use case? Choose 2 answers
Map the primary key from the two systems to party identification, using CRM ID as the identification name for individuals coming from the CRM, and Marketing ID as the identification name for individuals coming from the marketing platform.
Create a custom matching rule for an exact match on the Individual ID attribute.
*Create a matching rule based on party identification that matches on CRM ID as the party identification name.
*Map the primary key from the two systems to Party Identification, using CRM ID as the identification name for both.
Explanation: A. Mapping the primary key (CRM ID) from both systems to Party Identification ensures that both CRM Contacts and Marketing Contacts share the same identification method. This alignment using the CRM ID as the identification name for both parties is crucial for accurate identity resolution. D. Creating a matching rule based on party identification that specifically matches on the CRM ID as the party identification name reinforces the accuracy of matching CRM and Marketing Contacts. By setting up a matching rule that focuses on the CRM ID, you ensure that the identity resolution process prioritizes this common identifier for accurate matching.

Luxury Retailers created a segment targeting high value customers that it activates through Marketing Cloud for email communication. The company notices that the activated count is smaller than the segment count. What is a reason for this?
*Data Cloud enforces the presence of Contact Point for Marketing Cloud activations. If the individual does not have a related Contact Point, it will not be activated.
Marketing Cloud activations apply a frequency cap and limit the number of records that can be sent in an activation.
Marketing Cloud activations automatically suppress individuals who are unengaged and have not opened or clicked on an email in the last six months.
Marketing Cloud activations only activate those individuals that already exist in Marketing Cloud. They do not allow activation of new records.
Explanation: When activating segments from Data Cloud to Marketing Cloud for email communication, Data Cloud requires a related Contact Point to be present for each individual within the segment. If there's no associated Contact Point (which could be an email address or another identifiable contact information) for individuals within the segment, they won't be activated for email communication. This process ensures that activations are linked to reachable contact points, ensuring the effectiveness of the email communication strategy and avoiding sending emails to individuals without valid contact information.

Which permission setting should a consultant check if the custom Salesforce CRM object is not available in New Data Stream configuration?
*Confirm the View All object permission is enabled in the source Salesforce CRM org.
Confirm the Ingest Object permission is enabled in the Salesforce CRM org.
Confirm that the Modify Object permission is enabled in the Data Cloud org.
Confirm the Create object permission is enabled in the Data Cloud org.
Explanation: The "View All" permission in the source Salesforce CRM org allows users to view all records, including those they don't own. When setting up Data Streams in Salesforce or integrating data from Salesforce to another platform (like Data Cloud), it's crucial that the user configuring the Data Stream has the necessary permissions to access and view the object in Salesforce. If the user setting up the Data Stream lacks the "View All" permission for that object, it might not appear or be available for selection in the Data Stream configuration.

A consultant has an activation that is set to publish every 12 hours, but has discovered that updates to the data prior to activation are delayed by up to 24 hours. Which two areas should a consultant review to troubleshoot this issue? Choose 2 answers
*Review calculated insights to make sure they're run before segments are refreshed.
Review calculated insights to make sure they're run after the segments are retreshed.
* Review segments to ensure they're refreshed after the data is ingested.
Review data transformations to ensure they're run after calculated insights.
Explanation: <strong>Review segments to ensure they're refreshed after the data is ingested </strong> This step is crucial to ensure that segments are updated with the latest data after it's ingested. If segments aren't refreshed after new data ingestion, the activation process might use outdated segment definitions, causing delays in reflecting the most recent changes.</p></li><li><p><strong>Review calculated insights to make sure they're run before segments are refreshed </strong> If calculated insights, which are derived from the data, are not updated before segments are refreshed, the segments might not reflect the latest insights or changes derived from the data. Ensuring that calculated insights are updated before segments are refreshed helps in utilizing the most recent data insights for segmentation.</p></li></ul></div></div></form></div><div class="detailed-result-panel--panel-row--4lvVX detailed-result-panel--question-container--vh1KF"><form class="mc-quiz-question--container--dV-tK"><span>

Cumlus Financial uses data cloud to segment banking customers and activate them for direct mail via a cloud file storage activation. The company also wants to analyze individuals who have been in the segment within the last 2 years. which data cloud component allows for this?
nested segments
segment exclusion
Calculated insights
*segment membership data model object
Explanation: This data model object stores information about segment membership, including historical data, allowing organizations to query and analyze individuals' membership in particular segments over time. It enables businesses to understand the composition and characteristics of segments and the individuals within them, offering insights into how these segments have evolved or remained consistent over the specified period. Utilizing the Segment Membership Data Model Object, Cumulus Financial can effectively analyze and extract information about customers who were part of specific segments within the last 2 years, supporting their analytical and strategic decision-making processes.

Which two common use cases can be addressed with Data Cloud? Choose 2 answers
*Understand and act upon customer data to drive more relevant experiences.
*Harmonize data from multiple sources with a standardized and extendable data model.
Govern enterprise data lifecycle through a centralized set of policies and processes.
Safeguard critical business data by serving as a centralized system for backup and disaster recovery.
Explanation: <strong>Harmonizing Data from Multiple Sources :</strong> Data Cloud solutions often provide the infrastructure and tools necessary to aggregate, integrate, and standardize data from various sources. By employing a standardized and extendable data model, these platforms enable organizations to unify disparate data into a coherent format, facilitating analytics, reporting, and decision-making processes.</p></li><li><p><strong>Utilizing Customer Data for Personalized Experiences :</strong> Data Cloud solutions can assist in analyzing and leveraging customer data effectively. They provide capabilities to collect, store, process, and analyze vast amounts of customer-related information. This enables businesses to derive insights, understand customer behavior, preferences, and patterns, subsequently driving more personalized and relevant experiences for customers.</p></li></ul></div></div></form></div><div class="detailed-result-panel--panel-row--4lvVX detailed-result-panel--question-container--vh1KF"><form class="mc-quiz-question--container--dV-tK"><span>

Cumulus Financial uses Service Cloud as its CRM and stores mobile phone, home phone, and work phone as three separate fields for its customers on the Contact record. The company plans to use Data Cloud and ingest the Contact object via the CRM Connector. What is the most efficient approach that 4 consultant should take when ingesting this data to ensure all the different phone numbers are properly mapped and available for use in activation?
Ingest the Contact object and use streaming transforms to normalize the phone numbers from the Contact data stream into a separate Phone data lake object (DLO) that contains three rows, and then map this new DLO to the Contact Point Phone data map object.
Ingest the Contact object and then create a calculated insight to normalize the phone numbers, and then map to the Contact Point Phone data map object.
* Ingest the Contact object and map the Work Phone, Mobile Phone, and Home Phone to the Contact Point Phone data map object from the Contact data stream.
Ingest the Contact object and create formula fields in the Contact data stream on the phone numbers, and then map to the Contact Point Phone data map object.
Explanation: It involves organizing the phone numbers from the Contact object into a separate structured data object called a Phone Data Lake Object (DLO). This DLO contains three rows, one for each type of phone number (mobile, home, work). By doing this, it ensures that all the different phone numbers are properly organized and available for use in activation without the need for additional steps or complexity in data handling.

What should an organization use to stream inventory levels from an inventory management system into Data Cloud in a fast and scalable, near-real-time way?
Marketing Cloud Personalization Connector
*Ingestion API
Commerce Cloud Connector
Cloud Storage Connector
Explanation: The Ingestion API is the ideal choice for streaming inventory levels from an inventory management system into Data Cloud because it offers a direct, efficient, and optimized method specifically designed for data ingestion. It allows rapid, scalable, and near-real-time transmission of data from external sources, ensuring quick and reliable updates, such as inventory level changes, within the Data Cloud environment. This API facilitates a fast and seamless integration of external data into Data Cloud, maintaining a close-to-real-time representation of inventory data for analysis and other purposes.

A customer wants to use the transactional data from their data warehouse in Data Cloud. They are only able to export the data via an SFTP site. How should the file be brought into Data Cloud?
*Ingest the file with the SFTP Connector.
Manually import the file using the Data Import Wizard.
Use Salesforce's Dataloader application to perform a bulk upload from a desktop.
Ingest the file through the Cloud Storage Connector.
Explanation: SFTP (Secure File Transfer Protocol) is a standard method for securely transferring files over a network, and the SFTP Connector is specifically designed to handle data ingestion from SFTP sites. It allows seamless and secure transfer of files from the customer's data warehouse to the Data Cloud platform, ensuring the transactional data can be efficiently utilized within the Salesforce ecosystem.

What does it mean to build a trust-based, first-party data asset?
To obtain competitive data from reliable sources through interviews, surveys, and polls
To ensure opt-in consents are collected for all email marketing as required by law
*To provide transparency and security for data gathered from individuals who provide consent for its use and receive value in exchange
Te provide trusted, first-party data in the Data Cloud Marketplace that follows all cornpliance regulations
Explanation: Building a trust-based, first-party data asset revolves around collecting data directly from individuals who willingly provide their information with consent. This data collection process is transparent, ensuring individuals understand how their data will be used and providing them with security and control over their information. In exchange for sharing their data, individuals receive some form of value, whether it's personalized experiences, better services, or other benefits. This approach fosters trust between the data collector and the individuals, creating a reliable and ethical first-party data asset.

When performing segmentation or activation, which time zone is used to publish and refresh data?
Time zone of the user creating the activity
*Time zone set by the Salesforce Data Cloud org
Time zone of the Data Cloud Admin user
Time zone specified on the activity at the time of creation
Explanation: When performing segmentation or activation in Salesforce Data Cloud, the time zone used to publish and refresh data is typically the time zone set by the Salesforce Data Cloud org. This helps maintain consistency and ensures that data operations, scheduling, and updates are synced and aligned within the organizational context, irrespective of individual user time zones or activity creation time zones. This centralized time zone setting helps in managing data operations efficiently across the organization.

Cloud Kicks received a Request to be Forgotten by a customer. In which two ways should a consultant use Data Cloud to honor this request? Choose 2 answers
*Use the Consent API to suppress processing and delete the Individual and related records from source data streams.
Delete the data from the incoming data stream and perform a full refresh.
*Add the Individual ID to a headerless file and use the delete from file functionality.
Use Data Explorer to locate and manually remove the Individual.
Explanation: Using option B involves adding the Individual ID to a file without a header and leveraging Data Cloud's "delete from file" feature. This method allows specific records associated with the identified individuals to be deleted. Option D entails utilizing the Consent API, which enables the suppression of processing and deletion of the individual's data from source data streams within Data Cloud. This comprehensive approach ensures compliance with the request to be forgotten by stopping further processing and removing related records from the source data streams. Both methods effectively honor the customer's Request to be Forgotten within Data Cloud.
Cumulus Financial wants to segregate Salesforce CRM Account data based on Country for its Data Cloud users. What should the consultant do to accomplish this?
Use formula fields based on the Account Country field to filter incoming records.
*Use Salesforce sharing rules on the Account object to filter and segregate records based on Country.
Use the data spaces feature and apply filtering on the Account data lake object based on Country.
Use streaming transforms to filter out Account data based on Country and map to separate data model objects accordingly.
Explanation: Salesforce provides robust functionality to manage data access and visibility. By implementing sharing rules based on criteria such as Country, you can control and segregate access to Account records based on different criteria, including the Country field.This approach allows Cumulus Financial to leverage native Salesforce capabilities to control data access and visibility based on specific criteria like Country. It ensures that users only have access to the Account records associated with the designated Country, effectively segregating the data for Data Cloud users based on Country without physically separating the data or duplicating objects.

Cumulus Financial created a segment called High Balance C s. This is a fox that includes several criteria the ng tearm should consistently use. Which feature should the consultant suggest the marketing team use to ensure this consistency when creating future, more refined segments?
Create a High Investment Balance calculated insight.
Package High Investment Balance Customers in a data kit.
Create new segments using nested segments.
*Create new segments by cloning High Investment Balance Customers.
Explanation: Cloning the "High Investment Balance Customers" segment allows for the replication of the existing segment's criteria and conditions.This approach ensures consistency in the criteria used for segment creation. When the team clones the existing segment, they retain the preset criteria, eliminating the need to recreate the criteria manually. This helps maintain consistency in defining segments based on specific attributes or conditions, ensuring accuracy and uniformity across different segments.

Every day, Northern Trail Outfitters uploads a summary of the last 24 hours of store transactions to a new file in an Amazon S3 bucket, and files older than seven days are automatically deleted. Each file contains a timestamp in a standardized naming convention. Which two options should a consultant configure when ingesting this data stream? Choose 2 answers.
*Ensure the filename contains a wildcard to accommodate the timestamp
Ensure the refresh mode is set to "Full Refresh".
*Ensure the refresh mode is set to "Upsert"
Ensure that deletion of old files is enabled.
Explanation: <strong>Ensure the filename contains a wildcard to accommodate the timestamp. This is crucial to allow the system to recognize and ingest files based on the varying timestamps present in the filenames.Ensure the refresh mode is set to "Upsert". "Upsert" mode helps in updating existing records and inserting new ones. In this scenario, it might not directly relate to the deletion of old files but allows for efficient data updates in the system if changes occur in the existing files.

Northern Trail Outfitters (NTO), an outdoor lifestyle clothing brand, recently started a new line of business. The new business specializes in gourmet camping food. For business reasons as well as Security reasons, it’s important to NTO to keep all Data Cloud data separated by brand.Which capability best supports NTO's desire to separate its data by brand?
Data model objects for each brand
Date sources for each brand
Date streams for each brand
*Data spaces for each brand
Explanation: Data spaces provide distinct and isolated environments within Data Cloud, allowing for the segregation of data by creating separate spaces for different brands or business units. This segregation ensures that data from various brands or lines of business remains separate and independent within their designated spaces. It's an effective way to maintain data separation for security, compliance, and operational reasons, meeting NTO's requirement to keep Data Cloud data separated by brand.

A customer wants to use the transactional data from their data warehouse in Data Cloud. They are only able to export the data via an SFTP site. How should the file be brought into Data Cloud?
Use Salesforce’s Dataloader application to perform a bulk upload from a desktop.
*Ingest the file through the Cloud Storage Connector.
Ingest the file with the SFTP Connector.
Manually import the file using the Data Import Wizard.
Explanation: The Cloud Storage Connector is designed to integrate and ingest data from various cloud storage services, including SFTP. It allows seamless ingestion of data directly from cloud storage sources into Data Cloud without the need for manual or intermediary steps.By using the Cloud Storage Connector, the customer can set up a connection to their SFTP site within Data Cloud, enabling a direct and automated process to bring the transactional data into Data Cloud for further processing, analysis, or utilization. This method ensures efficient and direct ingestion of data from the SFTP site into the Data Cloud environment.

Northern Trail Outfitters wants to be able to calculate each customer's lifetime value (LTV) but also create breakdowns of the revenue sourced by website, mobile app, and retail channels.What should a consultant use to address this use case in Data Cloud?
Streaming data transform
Flow Orchestration
*Metrics on metrics
Nested segments
Explanation: Metrics on metrics allow for the creation of derived or calculated metrics within Data Cloud. This functionality enables the calculation of LTV for each customer while also breaking down the revenue by different channels such as website, mobile app, and retail.By utilizing this feature, the consultant can construct metrics that capture LTV per customer and further break down revenue metrics by the different channels, providing the necessary insights for both customer LTV and revenue segmentation by various sources.

Which two common use cases can be addressed with Data Cloud? Choose 2 answers.
*Harmonize data from multiple sources with a standardized and extendable data model.
Safeguard critical business data by serving as a centralized system for backup and disaster recovery.
Govern enterprise data lifecycle through a centralized set of policies and processes.
*Understand and act upon customer data to drive more relevant experiences.
Explanation: A. Understand and act upon customer data to drive more relevant experiences: Data Cloud platforms often allow businesses to gather, analyze, and derive insights from customer data. This facilitates creating personalized experiences, understanding customer behavior, and tailoring products or services to meet their needs effectively.<strong>C. Harmonize data from multiple sources with a standardized and extendable data model: Data Cloud solutions typically enable the integration and alignment of data from various sources into a unified, standardized format. This unified view aids in consistent analysis, reporting, and decision-making by ensuring data consistency and coherence across the organization.class="detailed-result-panel--panel-row--4lvVX detailed-result-panel--question-container--vh1KF">class="mc-quiz-question--container--dV-tK">

What is Data Cloud's primary value to customers?
*To provide a unified view of a customer and their related data
To connect all systems with a golden record
To create a single source of truth for all anonymous data
To create personalized campaigns by listening,understanding, and acting on customerbehavior
Explanation: The primary value of a Data Cloud to customers generally aligns with:A. To provide a unified view of a customer and their related dataData Cloud solutions aim to aggregate and integrate data from various sources to create a comprehensive, unified view of customers or entities. This unified view helps organizations gain insights into customer behaviour, preferences, and interactions across different touchpoints. It allows for better decision-making, personalized experiences, and more effective engagement strategies based on a holistic understanding of the customer.

A customer notices that their consolidation rate has recently increased. They contact the consultant to ask why. What are two likely explanations for the increase?Choose 2 answers
*New data sources have been added to Data Cloud that largely overlap with the existing profiles.
Identity resolution rules have been removed to reduce the number of matched profiles.
*Duplicates have been removed from source system data streams.
Identity resolution rules have been added to the ruleset to increase the number of matched profiles.
Explanation: Duplicates have been removed from source system data streams. Removing duplicates from the source system data streams can lead to a higher consolidation rate as there are fewer redundant or replicated records, making the matching and consolidation process more effective.New data sources have been added to Data Cloud that largely overlap with the existing profiles. Adding new data sources that contain overlapping or similar information to existing profiles can increase the consolidation rate by providing more data points to match and consolidate, thereby expanding the consolidated profiles within the system.

A customer needs to integrate in real time with Salesforce CRM. Which feature accomplishes this requirement?
*Data actions and Lightning web components
Streaming transforms
Data model triggers
Sales and Service bundle
Explanation: <strong>Data Actions: They allow you to perform operations like create, read, update, and delete (CRUD) on data in real time within the Data Cloud environment. These actions can be triggered by external events or through APIs, enabling real-time data interactions.Lightning Web Components: These are modern UI components built on the Lightning Web Components framework, which can be embedded within Salesforce CRM. They allow for the creation of dynamic and interactive interfaces that can seamlessly interact with the Data Cloud or other external systems, facilitating real-time data integration and interaction.Together, Data Actions and Lightning Web Components offer the capability to integrate and interact with Salesforce CRM in real time, providing the necessary tools to perform actions and display information dynamically within the CRM environment.

A user has built a segment in Data Cloud and is in the process of creating an activation. When selecting related attributes, they cannot find a specific set of attributes they know to be related to the individual. Which statement explains why these attributes are not available?
*The desired attributes reside on different related paths.
Activations can only include 1-to-1 attributes.
The segment is not segmenting on profile data.
The attributes are being used in another activation.
Explanation: <strong>Attributes residing on different related paths: In Data Cloud, related attributes are accessed based on paths or connections between different attributes. If the desired attributes are located on separate or different paths, they might not be available for selection when building an activation related to a specific segment.These related paths or connections determine how attributes are linked or associated within the system. If the desired attributes are not on the same or directly linked paths as the segment being used for activation, they might not be accessible or available for selection during the activation setup.This situation often arises when the attributes needed for activation are not directly connected or part of the same linked chain of related attributes as the segment being used, leading to the unavailability of these attributes during the activation configuration.

Northern Trail Outfitters wants to use some of its Marketing Cloud data in Data Cloud.Which engagement channe! data will require custom integration?
*SMS
Mobile push
Email
CloudPage
Explanation: SMS data might require custom integration due to the nature of the content and the way it's handled within Marketing Cloud. When moving SMS engagement data to Data Cloud, there might be specific requirements for formatting, handling opt-in/opt-out statuses, or managing timestamps and content encryption, which could necessitate custom integration efforts to ensure seamless ingestion and appropriate utilization within Data Cloud.

A client wants to bring in loyalty data from a custom object in Salesforce CRM thatcontains a point balance for accrued hotel points and airline points within the same record. Theclient wantstosplitthesepointsystemsintotwoseparaterecordsforbettertrackingandprocessing. What should a consultant recommend in this scenario?
*Create a junction object in Salesforce CRM and modify the ingestion strategy.
Clone the data source object.
Create a data kit from the data lake object and deploy it to the same Data- Cloud org.
Use batch transforms to create a second data lake object.
Explanation: In this scenario, the consultant should recommend:C. Create a junction object in Salesforce CRM and modify the ingestion strategy.By creating a junction object in Salesforce CRM, it becomes possible to split the loyalty data into separate records, allowing for better tracking and processing of hotel points and airline points independently within their respective records.Modifying the ingestion strategy alongside this creation will ensure that the data is processed accurately, reflecting the split nature of the loyalty points systems. This approach maintains the integrity of the data within Salesforce CRM and aligns with the client's requirement for distinct records for hotel and airline points.

What does the Ignore Empty Value option do in identity resolution?
Ignores empty fields when running the standard match rules
Ignores empty fields when running reconciliation rules
*Ignores Individual object records with empty fields when running identity resolution rules
Ignores empty fields when running any custom match rules
Explanation: In Data Cloud's identity resolution process, the "Ignore Empty Value" option specifically refers to ignoring Individual object records that have empty fields when running identity resolution rules.When this option is selected, the identity resolution process will exclude or ignore Individual records that contain empty fields from being considered or processed during the identity resolution rules. This helps in filtering out records lacking essential data, ensuring a more accurate and focused identity resolution process by disregarding incomplete or inadequate records.

Cloud Kicks received a Request to be Forgotten by a customer, In which two ways should a consultant use Data Cloud to honour this request? Choose 2 answers
Use Data Explorer to locate and manually remove the Individual.
*Add the Individual ID to a heacerless file and use the delete trom file functionality.
Delete the data from the incoming data stream and perform a full refresh.
*Use the Consent API to suppress processing and delete the individual and related records from source data streams.
Explanation: Use the Consent API to suppress processing and delete the individual and related records from source data streams: The Consent API allows for managing consent-related requests. Using it to suppress processing and delete individual records from the source data streams ensures compliance with the request by removing the specified individual's data. Add the Individual ID to a headerless file and use the delete from file functionality: This approach involves compiling the Individual ID into a file and using the "delete from file" functionality, likely available in Data Cloud, to facilitate the removal of the specified individual's data from the datasets.

A customer requests that their personal cata be deleted. Which action should the consultant take to accommodate this request in Data Cloud?
Use a strearning API call to delete the customer's information.
*Use the Data Rights Subject Request tool to request deletion of the customer's information.
Use Consent API to request deletion of the customer's information.
Use Profile Explorer to delete the customer data from Data Cloud.
Explanation: The Data Rights Subject Request tool is typically designed to handle data deletion requests and manage compliance with privacy regulations like GDPR or CCPA. This tool facilitates the processing of requests from individuals who want their personal information deleted from a system or database.By using the Data Rights Subject Request tool, the consultant can initiate the deletion process for the customer's personal data in compliance with the customer's request and relevant privacy regulations. It ensures proper handling of the deletion request within Data Cloud, maintaining compliance and fulfilling the customer's data privacy rights.

Northern Trail Outfitters wants to implement Data Cloud and has several use cases in mind.Which two use cases are considered a good fit for Data Cloud? Choose 2 answers
To eliminate the need for separate business intelligence and IT data management tools
*To ingest and unify date from various sources to reconcile customer identity
*To use harmonized data to more accurately understand the customer and business impact
To create and orchestrate cross-channel marketing messages
Explanation: To use harmonized data to more accurately understand the customer and business impact: Data Cloud is adept at harmonizing data from various sources, which can provide a unified view of customers. This unified data can then be used to gain deeper insights into customer behaviors, preferences, and their impact on business outcomes.To ingest and unify data from various sources to reconcile customer identity: Data Cloud specializes in ingesting and unifying data from multiple sources. This capability allows for the reconciliation of customer identity across disparate datasets, enabling a cohesive and accurate view of customer profiles.

How can a consultant modify attribute names to match a naming convention in Cloud File Storage targets?
Use a formula field to update the field name in an activation.
Update field names in the data model object.
*Update attribute names in the data stream configuration.
Set preferred attribute names when configuring activation.
Explanation: When dealing with Cloud File Storage targets, the attribute names can be modified to match a specific naming convention by updating the attribute names within the data stream configuration. This process allows consultants to align the attribute names as per the desired naming convention before the data is stored in the Cloud File Storage target. Adjusting attribute names at the data stream configuration level ensures that the data stored follows the prescribed naming conventions without impacting the original data source or the data model object.

A retailer wants to unify profiles using Loyalty ID which is different than the unique ID of their customers. Which object should the consultant use in identity resolution to perform exact match rules on the Loyalty ID?
Contact Identification object
Individual object
Loyalty Identification object
*Party Identification object
Explanation: The Party Identification object within identity resolution is designed to manage and resolve identities across various identifiers. When a retailer aims to unify profiles using a Loyalty ID that differs from the unique customer ID, using the Party Identification object allows for the creation of precise rules for exact match resolutions based specifically on Loyalty IDs. This object is tailored to handle diverse identifiers, ensuring accurate matching criteria for unifying profiles based on the Loyalty ID as a specific identifier.

A customer is trying to activate data from Data Cloud to an Amazon S3 Cloud File Storage Bucket. Which authentication type should the consultant recommend to connect to the S3 bucket from Data Cloud?
Use an S3 Encrypted Username and Password.
*Use an S3 Access Key and Secret Key.
Use a JWT Token generated on S3.
Use an S3 Private Key Certificate.
Explanation: This method involves providing an Access Key ID and Secret Access Key generated from the AWS Identity and Access Management (IAM) service. These keys are used to securely authenticate and access resources within the S3 bucket. It's a standard and widely accepted way to authenticate access to AWS services like S3.

A client wants to bring in loyalty data from a custom object in Salesforce CRM that contains a point balance for accrued hotel points and airline points within the same record, The client wants to split these point systems into two separate records for better tracking and processing.What should 2 consultant recommend in this scenario?
Clone the data source object.
Use batch transforms to create a second data lake object.
Create 8 data kit from the date lake object and deploy it to the same Data Cloud org.
* Create a junction object in Salesforce CRM and modify the ingestion strategy.
Explanation: By creating a junction object in Salesforce CRM, the client can establish a separate entity to handle the two types of loyalty points (hotel points and airline points). Modifying the ingestion strategy to incorporate this junction object allows for a more structured and organized way to manage and process the loyalty point data separately.

What does the Source Sequence reconciliation rule do in identity resolution?
*Identifies which data sources should be used in the process of reconciliation by prioritizing the most recently updated data source
Identifies which individual records should be merged into a unified profile by setting a priority for specific data sources
Includes data from sources where the data is most frequently occurring
Sets the priority of specific data sources when building attributes in a unified profile, such as a first or last name
Explanation: This rule focuses on determining the most recently updated or refreshed data source to prioritize its information during the reconciliation process, ensuring that the latest and most relevant data is used when consolidating or reconciling different sources into a unified profile.

Which two common use cases can be addressed with Data Cloud? Choose 2 answers
Govern enterprise data lifecycle through a centralized set of policies and processes.
*Harmonize data from multiple sources with a standardized and extendable data model.
Safeguard critical business date by serving as a centralized system for backup and disaster recovery.
*Understand and act upon customer data to drive more relevant experiences.
Explanation:  Understand and act upon customer data to drive more relevant experiences: Data Cloud allows businesses to aggregate, unify, and analyze customer data from various sources. This unified view enables better insights into customer behaviour, preferences, and interactions, empowering businesses to personalize their services and offerings, resulting in more relevant and targeted customer experiences.Harmonize data from multiple sources with a standardized and extendable data model: Data Cloud offers the capability to consolidate and harmonize data from disparate sources into a centralized platform using a standardized data model. This harmonization process ensures that data from various systems or sources is aligned, standardized, and accessible in a uniform structure, enabling better analysis, reporting, and decision-making across the organization.

A customer wants to create segments of users based on their Customer Lifetime Value. However, the source data that will be brought into Data Cloud does not include that key performanceindicator (KPI). Which sequence of steps should the consultant follow to achieve this requirement?
Ingest Date &gt; Create Calculated Insight &gt; Map Data to Data Model &gt; Use in Segmentation
Ingest Data &gt; Map Data to Data Model &gt; Create Calculated Insight &gt; Use in Segmentation
Create Calculated Insight &gt; Ingest Data &gt; Map Data to Data Model &gt; Use in Segmentation
*Create Calculated Insight &gt; Map Data to Data Model &gt; Ingest Data &gt; Use in Segmentation
Explanation: Create Calculated Insight: Initially, you create a calculated insight within Data Cloud. This insight will derive or calculate the Customer Lifetime Value from available data or existing KPIs.<strong>Map Data to Data Model: Once the calculated insight is created, you map this new insight to the data model within Data Cloud. This step integrates the derived CLV into the existing data structure, making it available for segmentation.<strong>Ingest Data: Subsequently, you ingest or import the updated data, which now includes the calculated Customer Lifetime Value, into Data Cloud.<strong>Use in Segmentation: Finally, after the CLV is integrated into the data model, you can utilize this data attribute in segmentation to create segments based on Customer Lifetime Value.This sequence ensures that the necessary KPI (Customer Lifetime Value) is derived or calculated within Data Cloud, mapped into the data structure, integrated into the dataset through data ingestion, and finally utilized for segmentation purposes.

A consultant is integrating an Amazon S3 activated campaign with the customer's destination system. In order for the destination system to find the metadata about the segment, which file on the $3 will contain this information for processing?
The .txt file
*The .json file
The .zip file
The .csv file
Explanation: The .json file, contains the metadata about the segment. JSON (JavaScript Object Notation) files are commonly used for storing structured data, including metadata, due to their simplicity and flexibility in representing data. In the context of integrating an Amazon S3 activated campaign with a destination system, the .json file would likely contain the necessary metadata, such as information about the segment's structure, attributes, or other relevant details needed for processing within the destination system.

Northern Trail Outfitters (NTO) wants to send a i ign for ci thar have within the past 6 months. The created &amp; to meet this Now, NTO brings an to who have made purchases within the last week.What should the consultant use to remove the recent customers?
Strearming insights
Batch transforms
*Segmentation exclude rules
Related attributes
Explanation: Segmentation exclude rules allow for the exclusion of specific criteria or conditions from a segment. In this case, to remove the recent customers who have made purchases within the last week from the segment of customers who made purchases in the past 6 months, segmentation exclude rules can be applied to exclude the recent customers based on their purchase timeframe.This approach enables the consultant to refine the segment by excluding the specific subset of recent customers, ensuring that the segment includes only customers who made purchases within the past 6 months while excluding those who made recent purchases within the last week.

During discovery, which feature should a consultant highlight for a customer who has multiple data sources and needs to match and reconcile data about individuals into a single unified profile?
* Identity Resolution
Date Cleansing
Data Consolidation
Harmonization
Explanation: Identity Resolution is a crucial feature that enables the merging and unification of scattered data about individuals across diverse data sources into a singular, comprehensive profile. It works by matching and reconciling data from various sources to identify and link records belonging to the same person. By using unique identifiers or attributes, this process accurately identifies and connects disparate records, creating a unified view of an individual. This capability allows organizations with multiple data sources to consolidate fragmented information, ensuring a more holistic understanding of customers or entities by creating a single, comprehensive profile that spans across various data sets.

A Data Cloud customer wants to adjust their identity resolution rules to increase their accuracy of matches. Rather than matching on email address, they want to review a rule that joins theirCRM Contacts with their Marketing Contacts, where both use the CRM ID as their primary key.Which two steps should the consultant take to address this new use case?Choose 2 answers
* Create a matching rule based on party identification that matches on CRM ID as the party identification name.
* Map the primary key from the two systems to party identification, using CRM ID as the identification name for individuals coming from the CRM, and Marketing ID as theidentification name for individuals coming from the marketing platform
Create a custom matching rule for en exact match on the Individual 1D attribute.
 Map the primary key from the two systems to Party Identification, using CRM ID as the identification name for both.
Explanation:  Map the primary key from the two systems to party identification, using CRM ID as the identification name for individuals coming from the CRM, and Marketing ID as the identification name for individuals coming from the marketing platform. This step involves mapping the primary keys (CRM ID and Marketing ID) from both systems to the party identification within the identity resolution process. It sets up the identification names for individuals from each system, enabling accurate matching based on these IDs. Create a matching rule based on party identification that matches on CRM ID as the party identification name. This step involves creating a matching rule within the identity resolution process that specifically matches based on party identification names, focusing on CRM ID as the key for matching individuals from both CRM Contacts and Marketing Contact

A user is not seeing suggested values from newly-modeled data when building a segment. What is causing this issue?
*Value suggestion is still processing and takes up to 24 hours to be available.
Value suggestion can only work on direct attributes and not related attributes.
Value suggestion requires Data Aware Specialist permissions at a minimum.
Value suggestion will only return results for the first 50 values of a specific attribute.
Explanation: When new data is modeled or added to the system, it often needs time to undergo processing and indexing before value suggestions become available. This processing period might take up to 24 hours before the system can suggest values for attributes that have been recently added or modified.During this processing time, the system is organizing and indexing the data, making it ready for use in features like value suggestion. So, if a user isn't seeing suggestions for values with newly-modeled data, it could be because the system is still processing the information, and it may take some time before the suggestions become available for use in segmentation.

What is Data Cloud's primary value to customers?
* To provide a unified view of a customer and their related data
To connect all systems with a golden record
To create personalized campaigns by listening, understanding, and acting on customer behaviour
To create a single source of truth for all anonymous data
Explanation: This is the core essence of Data Cloud's functionality. It aggregates and harmonizes data from various sources, allowing businesses to create a holistic view of their customers by consolidating and unifying data scattered across multiple systems. It enables businesses to understand their customers better, creating a comprehensive and unified profile that incorporates various aspects of their interactions and behaviours. This unified view helps in making informed decisions, personalized marketing, and improving overall customer experiences.

A consultant is discussing the benefits of Data Cloud with 2 customer that has multiple disjointed data sources. Which two functional areas should the consultant highlight in relation to managing customer data? Choose 2 answers
*Unified Profiles
Master Data Management
*Date Harmonization
Data Marketplace
Explanation: <strong>Unified Profiles: This feature allows for the creation of comprehensive customer profiles by consolidating data from various sources. It enables a single, holistic view of customers, combining information from disparate systems to understand and interact with customers more effectively.Data Harmonization: With Data Cloud, harmonizing data from different sources is achievable. It facilitates the process of aligning and standardizing varied datasets into a common structure or format. This harmonization ensures consistency and uniformity in the data, making it easier to analyze, query, and derive insights.By emphasizing Unified Profiles and Data Harmonization, the consultant highlights the capability of Data Cloud to integrate data sources efficiently, creating a unified view of customers while ensuring data consistency and accuracy across the board.

A retailer wants to unify profiles using Loyalty ID which is different than the unique ID of their customers. Which object should the consultant use in identity resolution to pertorm exact match rules on the Loyalty ID?
Party Identification object
Contact Identification object
*Loyalty Identification object
Individual object
Explanation: Using the Loyalty Identification object allows the consultant to specifically target and perform exact match rules on the Loyalty ID, which is distinct from the unique ID used for customers. This object serves as the specific field or attribute where the Loyalty ID information is stored, enabling the exact matching process based on this particular identifier.

Which configuration supports separate Amazon S3 buckets for data ingestion and activation?
Dedicated S3 data sources in activation setup
Separate user credentials for data stream and activation target
*Dedicated S3 data sources in Data Cloud setup
Multiple S3 connectors in Data Cloud setup
Explanation: When dealing with Amazon S3 buckets for data management within Data Cloud, having dedicated S3 data sources in the Data Cloud setup allows for distinct and isolated configurations. This means you can specify separate S3 sources for ingestion (bringing data into Data Cloud) and activation (sending data out of Data Cloud).By setting up dedicated S3 data sources, you can ensure that the data ingestion process is distinct from the data activation process. It helps maintain data integrity and separation, enabling you to manage and control data flows independently for ingestion into Data Cloud and activation out of Data Cloud. This separation is beneficial for various reasons, including security, control, and ease of managing different stages of data handling within the Data Cloud platform.

Which data model subject area should be used for any Organization, Individual, or Memberin the Customer 360 data model?
Engagement
Global Account
Membership
*Party
Explanation: The "Party" subject area typically covers a wide range of data related to individuals, organizations, or any entity that interacts with the organization. It includes demographic information, contact details, preferences, and other relevant information about the customer or member. This subject area forms the foundational element in many Customer 360 models as it captures the basic details of the involved parties.

Which permission setting should a consultant check if the custom Salesforce CRM object is not available in New Data Stream configuration?
Confirm that the Modify Object permission is enabled in the Data Cloud org.
Confirm the Ingest Object permission is enabled in the Salesforce CRM org.
Confirm the Create Object permission is enabled in the Data Cloud org.
* Confirm the View All object permission is enabled in the source Salesforce CRM org.
Explanation: This permission ensures that the Data Cloud user has the necessary access rights to view all objects within the source Salesforce CRM organization. If this permission is not enabled, it might restrict the visibility of certain objects, including custom objects, within the Data Cloud configuration interface, leading to the absence of the custom object in the New Data Stream setup. Hence, verifying and enabling the "View All" object permission in the source Salesforce CRM org is crucial for visibility and selection of custom objects during Data Stream configuration.

During a privacy law discussion with a customer, the customer indicates they need to honor requests for the right to be forgotten. The consultant determines that Consent API will solve this business need. Which two considerations should the consultant inform the customer about? Choose 2 answers
Data deletion requests are processed within 1 hour.
Data deletion requests are reprocessed at 30, 60, and 90 days.
*Data deletion requests submitted to Data Cloud are passed to all connected Salesforce clouds.
*Data deletion requests are submitted for Individual profiles.
Explanation: Data deletion requests are submitted for Individual profiles: The Consent API typically processes data deletion requests at an individual profile level. This means that when a request for the right to be forgotten is made, it pertains specifically to the individual's profile or data record within the system.Data deletion requests submitted to DataCloud are passed to all connected Salesforce clouds: The Salesforce clouds connected to DataCloud, such as Sales Cloud, Service Cloud, Marketing Cloud, etc., are part of the Salesforce ecosystem. When a data deletion request is submitted through the Consent API in DataCloud, it triggers a cascade effect where the request is transmitted to all connected Salesforce clouds, ensuring that the individual's data is deleted consistently across the entire Salesforce environment.

An organization wants to enable users with the ability to identity and select text attributes from a picklist of options. Which Data Cloud feature should help with this use case?
Data harmonization
Global picklists
Transformation formulas
* Value suggestion
Explanation: The "Value suggestion" feature helps by suggesting and predicting values based on historical data or predefined patterns, assisting users in selecting appropriate text attributes from a list of suggested options. This functionality aids in streamlining data entry, improving accuracy, and facilitating the selection of values from a set of suggested options.

Northern Trail Outfitters (NTO) creates a calculated insight to compute recency, monetary (RFM) scores on its unified individuals. NTO then creates a segment based on these scores that it activates to a Marketing Cloud activation target. Which two actions are required when configuring the activation? Choose 2 answers
*Select contact points.
Choose a segment.
Add additional attributes.
*Add the calculated insight in the activation.
Explanation: <strong>Add the calculated insight in the activation When activating a segment based on calculated insights (such as RFM scores), it's essential to include these insights in the activation setup. This ensures that the segment generated from RFM scores is utilized during the activation process, allowing for targeted actions based on these insights.Select contact points For activations involving Marketing Cloud, it's crucial to select the contact points, such as email addresses or other communication channels, through which the targeted individuals will be reached. Selecting appropriate contact points ensures that the activation reaches the intended audience via the chosen communication channels.These actions—adding the calculated insight in the activation and selecting the contact points—ensure that the RFM-based segment is included and targeted effectively in the activation process toward the Marketing Cloud activation target.

Data Cloud receives a nightly file of all ions from the ious day. Several and activations depend upon insights from the updated data in order to maintain accuracy in the customer's scheduled comapign messages .What should the consultant do to ensure the ecommerce data is ready for use for each of the scheduled activations?
Set a refresh schedule for the calculated insights to occur every hour.
Ensure the activations are set to Activation and i Publish every hour.
* Use Flow to trigger a change data event on the ecommerce data to refresh insights and before the are to run.
Ensure the segments are set to Rapid Publish and set to refresh every hour.
Explanation: By using Flow to trigger a change data event on the ecommerce data, it initiates a process that refreshes the insights associated with that data. This ensures that the insights utilized in scheduled activations are updated and accurate before the activations occur. Triggering this event just before the activations are set to run allows for the most recent data insights to be available, maintaining accuracy in the customer's scheduled campaign messages.

A user wants to be able to create a multi-dimensional metric to identify unified individual lifetime value (LTV). Which sequence of data mode! object (DMO) joins is necessary within the calculated insight to enable this calculation?
Sales Order &gt; Unified Individual
* Unified Individual &gt; Unified Link Individual &gt; Sales Order
Unified Individual &gt; Individual &gt; Sales Order
Sales Order &gt; Individual &gt; Unified Individual
Explanation: This sequence suggests joining the "Unified Individual" data model object with the "Unified Link Individual" data model object, and then further connecting it to the "Sales Order" data model object. This progression likely establishes a linkage between unified individuals and their linked records, and then extends it to the sales order data, allowing for the calculation of lifetime value (LTV) within this specific data model structure.

Cumulus Financial created a called Multiple that i ir who have invested in two or more mutual funds.‘The company plans to send an email to this segment regarding a new mutual fund offering, and wants to personalize the email content with information about each customer's current mutualfund investments. How should the Data Cloud consultant configure this activation?
Include Fund Type equal to “Mutual Fund” as a related attribute. Configure an activation based on the new segment with no additional attributes.
Include Fund Name and Fund Type by default for post processing in the target system.
*Choose the Multiple Investments segment, choose the Email contact point, and add related attribute Fund Type.
Choose the Multiple Investments segment, choose the Email contact point, add related attribute Fund Name, and add related attribute filter for Fund Type equal to “Mutual Fund”.
Explanation: It selects the specific segment of customers who have invested in multiple funds.By choosing the Email contact point, it designates the communication channel for this activation (in this case, email).Adding the related attribute Fund Type allows for the inclusion of this attribute in the activation, potentially providing information about the types of funds the customers have invested in.

Which two steps should a consultant take if a successfully configured Amazon S3 data stream fails to refresh with a "NO FILE FOUND" error message?Choose 2 answers
Check if the Amazon S3 data source is enabled in Data Cloud Setup.
*Check If the file exists in the specified bucket location.
Check if correct permissions are configured for the Data Cloud user.
*Check if correct permissions are configured for the S3 user.
Explanation: C. Check If the file exists in the specified bucket location: Verifying whether the file exists in the specified Amazon S3 bucket location is crucial when encountering a "NO FILE FOUND" error message during a data stream refresh. This step helps confirm if the expected file is present in the designated location. If the file is missing or located elsewhere, it would result in the failure to find the file during the refresh process.D. Check if correct permissions are configured for the S3 user: Ensuring that the appropriate permissions are set up for the Amazon S3 user is vital. The S3 user associated with the Data Cloud needs to have the necessary access and permissions to retrieve data from the specified bucket. Inadequate permissions could lead to failure in accessing or retrieving the file during the refresh process, resulting in a "NO FILE FOUND" error.

A customer is concemed that the consolidation rate displayed in the identity resolution is quite low compared to their initial estimations. Which configuration change should a consultant consider in order to increase the consolidation rate?
Change reconciliation rules to Most Occuring.
*Include additional attributes in the existing matching rules.
Reduce the number of matching rules.
Increase the number of matching rules.
Explanation: Increasing the consolidation rate often involves refining the matching process by incorporating more attributes or criteria into the existing matching rules. By including additional attributes in the matching rules, you're broadening the criteria for identifying and linking records, which can lead to a higher consolidation rate.Expanding the attributes used in the matching rules can improve the accuracy and inclusiveness of the matching process, increasing the likelihood of correctly identifying and consolidating records that might have been missed with fewer or narrower criteria.

What should a user do to pause a segment activation with the intent of using that seqment again?
Deactivate the segment.
Skip the activation.
* Stop the publish schedule.
Delete the segment.
Explanation: Stopping the publish schedule might refer to halting or pausing the scheduled activations of the segment. This action could effectively pause the segment's activation by stopping its scheduled publishing or activation at specified intervals.

What should an organization use to stream inventory levels from an inventory management system into Data Cloud in a fast and scalable, near-real-time way?
Commerce Cloud Connector
*Ingestion API
Marketing Cloud Personalization Connector
Cloud Storage Connector
Explanation: The Ingestion API facilitates the seamless and real-time ingestion of data into Data Cloud from various sources. It allows for direct data streaming and is designed to handle high volumes of data, ensuring efficient and rapid ingestion, making it an ideal choice for near-real-time data updates, such as inventory levels.

A segment fails to refresh with the error "Segment references too many data lake objects (DLOS)". Which two troubleshooting tips should help remedy this issue?Choose 2 answers
*Split the segment into smaller segments.
*Use calculated insights in order to reduce the complexity of the segmentation query.
Refine segmentation criteria to limit up to five custom data model objects (DMOs).
Space out the segment schedules to reduce DLO load.
Explanation: A. Split the segment into smaller segments: Breaking down the segment into smaller, more manageable segments can help reduce the complexity and the number of data lake objects referenced in each individual segment. By dividing the segment, you distribute the load across multiple segments, potentially alleviating the issue.B. Use calculated insights to reduce the complexity of the segmentation query: Calculated insights can pre-calculate certain complex computations or aggregations, reducing the complexity of the segmentation query itself. By utilizing calculated insights, you might decrease the number of data lake objects referenced or optimize the query's computational load, thus mitigating the error related to excessive DLOs.Both of these approaches aim to reduce the complexity and load associated with the segment's query or composition, thereby addressing the issue of referencing too many data lake objects and potentially resolving the segment refresh failure.

Which statement about Data Cloud's Web and Mobile Application Connector is true?
The connector schema can be updated to delete an existing field.
Any data streams associated with the connector will be automatically deleted upon deleting the app from Data Cloud Setup.
*A standard schema containing event, profile, and transaction data is created at the time the connector is configured.
The Tenant Specific Endpoint is auto-generated in Data Cloud when setting the connector.
Explanation: This connector typically establishes a structured schema that encompasses various types of data related to events, profiles, and transactions from web and mobile applications. This standardized schema formation occurs during the setup of the connector to facilitate the integration and ingestion of these different types of data into Data Cloud.

Which two steps should a consultant take if a successfully contiqured Amazon $3 data stream fails to refresh with a “NO FILE FOUND” error message?Choose 2 answers
Check if correct permissions are configured for the Data Cloud user.
Check if the Amazon S3 data source is enabled in Data Cloud Setup.
*Check if the file exists in the specified bucket location.
*Check if correct permissions are configured for the S3 user.
Explanation: Check if the file exists in the specified bucket location. This step involves verifying if the file that the data stream is supposed to refresh from actually exists in the designated bucket location. The "NO FILE FOUND" error could occur if the file was not properly uploaded or if the path specified in the data stream configuration does not match the actual location of the file.Check if correct permissions are configured for the S3 user. Ensuring that the Data Cloud user has the appropriate permissions to access and retrieve data from the specified Amazon S3 bucket is crucial. Incorrect or insufficient permissions granted to the S3 user used for the data stream configuration can lead to failure in accessing the file, resulting in the "NO FILE FOUND" error during refresh attempts.

A consultant has an activation that is set to publish every 12 hours, but has discoveredthat updates to the data prior to activation are delayed by up to 24 hours.Which two areas should a consultant review to troubleshoot this issue?Choose 2 answers
Review calculated insights to make sure they're run before segments are refreshed.
Review segments to ensure they're refreshed after the data is ingested.
*Review calculated insights to make sure they're run after the segments are refreshed.
*Review data transformations to ensure they're run after calculated insights.
Explanation: Data Transformations and Calculated Insights: If data transformations are running before calculated insights, it might mean that insights are generated from outdated data. Ensuring that data transformations occur after insights are calculated ensures the most current information is used in the insights.<strong>Segment Refresh Timing: If segments are being refreshed before the data is completely ingested or updated, the segments might be using outdated information. Refreshing segments after data ingestion ensures that the segments are updated with the most recent data.class="detailed-result-panel--panel-row--4lvVX detailed-result-panel--question-container--vh1KF">class="mc-quiz-question--container--dV-tK">

A customer is trying to activate data from Data Cloud to an Amazon $3 Cloud File Storage Bucket. Which authentication type should the consultant recommend to connect to the $3 bucket from Data Cloud?
Use a JWT Token generated on S3.
* Use an S53 Access Key and Secret Key.
Use an 53 Private Key Certificate.
Use an S3 Encrypted Username and Password.
Explanation: This method involves using an Access Key and a Secret Key, which are standard credentials provided by Amazon S3 to access and authenticate connections to the storage bucket. These keys grant permission for Data Cloud to securely access and interact with the S3 bucket, ensuring a secure and authenticated connection for data activation and transfer.

Northern Trail Outfitters (NTO) wants to connect their B2C Commerce data with Data Cloud and bring two years of transactional history into Data Cloud. What should NTO use to achieve this?
Direct Sales Order entity ingestion
*B2C Commerce Starter Bundles plus a custom extract
Direct Sales Product entity ingestion
B2C Commerce Starter Bundles
Explanation: The B2C Commerce Starter Bundles, along with a custom extract, provide a comprehensive approach to connect B2C Commerce data with Data Cloud. These bundles often include pre-built connectors and tools designed to streamline the integration process between B2C Commerce platforms and Data Cloud. Additionally, supplementing this with a custom extract allows NTO to specifically extract and incorporate the two years of transactional history into Data Cloud, ensuring the desired historical data is included in their dataset for analysis and segmentation purposes.

Cumulus Financial is currently using Data Cloud and nal data trom its b system vis an S3 Connector in upsert mode. During the initial setup six months ago, the company created a formula field in Data Cloud to create a custom classification. It now needs to update this formula to account for more classifications. What should the consultant keep in mind with regard to formula field updates when using the S3 Connector?
Data Cloud does not support formule field updates for data streams of type upsert.
Date Cloud will only update the formula on 8 go-forward basis for new records.
*Date Cloud will inibate 2 full refresh of data from S3 and will update the formula on all records.
Data Cloud will update the formula for all records at the next incremental upsert refresh.
Explanation: When updating formula fields in Data Cloud, especially with an S3 Connector in upsert mode, the system typically initiates a process that involves refreshing the data from the source (in this case, from S3). The update to the formula field will occur during this refresh process. For a complete update across all records, the system might require full refreshes to ensure the changes are applied universally to the existing data.This means that the consultant should anticipate the need for multiple refresh cycles or iterations (likely two full refreshes) to ensure the updated formula field logic is applied to all existing records sourced from S3.

A Data Cloud customer wants to adjust their identity resolution rules to increase their accuracy of matches. Rather than matching on email address, they want to review a rule that joins their CRM Contacts with their Marketing Contacts, where both use the CRM ID as their primary key.Which two steps should the consultant take to address this new use case?Choose 2 answers
*Map the primary key from the two systems to Party Identification, using CRM ID as the identification name for both.
*Create a matching rule based on party identification that matches on CRM ID as the party identification name.
Create a custom matching rule for an exact match on the Individual ID attribute.
Map the primary key from the two systems to party identification, using CRM ID as the identification name for individuals coming from the CRM, and Marketing ID as the identification name for individuals coming from the marketing platform.
Explanation: A. Mapping the primary key (CRM ID) from both systems to Party Identification ensures that both CRM Contacts and Marketing Contacts share the same identification method. This alignment using the CRM ID as the identification name for both parties is crucial for accurate identity resolution.D. Creating a matching rule based on party identification that specifically matches on the CRM ID as the party identification name reinforces the accuracy of matching CRM and Marketing Contacts. By setting up a matching rule that focuses on the CRM ID, you ensure that the identity resolution process prioritizes this common identifier for accurate matching.

A customer requests that their personal data be deleted. Which action should the consultant take to accommodate this request in Data Cloud?
Use Consent API to request deletion of the customer's information.
*Use the Data Rights Subject Request tool to request deletion of the customer's information.
Use a streaming API call to delete the customer's information.
Use Profile Explorer to delete the customer data from Data Cloud.
Explanation: This tool is designed specifically for handling data subject requests, such as requests for data deletion. It allows the consultant to manage and process requests related to data rights, including deletion requests, in accordance with privacy regulations and company policies. Using this tool ensures that the customer's request is appropriately handled within the DataCloud system.

During discovery, which feature should a consultant highlight for a customer who has multiple data sources and needs to match and reconcile data about individuals into a single unified profile?
Harmonization
Data Consolidation
*Identity Resolution
Data Cleansing
Explanation: This feature involves the process of matching and consolidating data from various sources to identify and link records belonging to the same individual. It focuses on accurately determining that multiple records across datasets indeed represent the same person. This process is fundamental in creating a unified profile by resolving and consolidating disparate data into a single cohesive representation for each individual.Identity Resolution helps in removing duplicates, reconciling conflicting information, and creating a comprehensive view of an individual across various data sources. Therefore, for a customer dealing with multiple data sources and aiming to create unified profiles, Identity Resolution becomes a crucial feature to highlight during discovery.

To import campaign members into 2 campaign in Salesforce CRM, 2 user wants to export the segment to Amazon 53. The resulting file needs to include the Salesforce CRM Campaign ID in the name. What are two ways to achieve this outcome? Choose 2 answers:
*Include campaign identifier in the activation name
Include campaign identifier in the segment name
Hard code the campaign identifier as a new attribute in the campaign activation.
*Include campaign identifier in the filename specification
Explanation:  Including the campaign identifier in the activation name. This action ensures that when the segment is exported, the activation name encompasses or incorporates the Campaign ID, making it identifiable within the exported file or folder structure.Including the campaign identifier in the filename specification. By doing so, when the segment is exported to Amazon S3, the file naming convention incorporates the Campaign ID, making it part of the exported file's name, aiding in easy identification and organization of exported data files.

A consultant is working in a customer's Data Cloud org and is asked to delete the existing identity resolution ruleset. Which two impacts should the consultant communicate as a result of this action? Choose 2 answers
*Unified customer data associated with this ruleset will be removed.
All source profile data will be removed.
*All individual data will be removed.
Dependencies on data model objects will be removed.
Explanation: Unified customer data associated with this ruleset will be removed. When an identity resolution ruleset is deleted, the association between the ruleset and the unified customer data processed through it is typically removed as well. This means that the organization might lose the unified view of customers based on the ruleset's specifications.All individual data will be removed. Deleting the identity resolution ruleset could impact the individual data processed or organized based on those rules. It might remove the data categorization or organization carried out through that specific ruleset, potentially affecting how individual data was identified or associated with each other.Communicating these impacts is crucial as deleting an identity resolution ruleset could result in the loss or disruption of data associations, organization, or categorization tied to the ruleset, affecting the unified view of customers and individual data within Data Cloud.

When performing segmentation or activation, which time zone is used to publish and refresh data?
Time zone of the user creating the activity
Time zone specified on the activity at the time of creation
Time zone of the Data Cloud Admin user
*Time zone set by the Salesforce Data Cloud org
Explanation: The org-level time zone settings can dictate the time zone used for various automated processes, including scheduling activities like data refresh or segmentation within the Data Cloud environment. This configuration ensures consistency in timing and execution based on the organization's specified time zone preferences. Time zone set by the Salesforce Data Cloud org—can indeed determine the time zone used for scheduling and refreshing data in Salesforce Data Cloud activities.

A consultant wants to build a new audience in Data Cloud. Which three criteria can the consultant include when building a segment? Choose 3 answers
*Direct attributes
Calculated insights
*Related attributes
*Data stream attributes
Streaming insights
Explanation: Direct attributes: These are the standard and direct data attributes available within the system. These could be basic demographic details, transactional data, or any directly captured information about individuals.<strong>Datastream attributes: These attributes are sourced directly from the ingested data streams. They encompass information collected from various sources and can be utilized to build segments based on the specific characteristics or behaviors captured from these streams.<strong>Related attributes: Related attributes are those linked or associated with the primary attributes. For instance, if you're focusing on customer purchases, related attributes might include purchase history, preferences, or items frequently bought together. Including these related attributes allows for a more comprehensive segmentation strategy.These criteria cover a wide range of data sources and characteristics, from the directly captured attributes to data sourced from streams and related attributes linked to the primary ones. Including them enables a thorough and nuanced approach to building audiences or segments within Data Cloud.

Luxury Retailers created a segment targeting high value customers that it activates through ing Cloud for email The notices that the activated count is smatier than the segment count, What is a reason for this?
Cloud i i who are and have not opened or clicked on an email in the last six months.
*Data Cloud enforces the presence of Contact Point for Cloud If the I does not have a related Contact Point, it will not be activated.
Marketing Cloud activabons only activate those individuals that already exist in Marketing Cloud. They do not allow activation of new records.
Cloud apply &amp; cap and limut the number of records that can be sent in an activaton.
Explanation: Data Cloud often requires the presence of a Contact Point (such as an email address) for activation. If the individuals within the segment lack a related Contact Point (for instance, an email address), they might not be activated or included in the activation process. This condition can result in a smaller activated count compared to the segment count, especially if some individuals within the segment don't have the necessary Contact Points for email activation.